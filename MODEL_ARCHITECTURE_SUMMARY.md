# FedDWA è”é‚¦å­¦ä¹ æ¡†æ¶ - æ¨¡å‹æ¶æ„æ€»ç»“

## ğŸ“š é¡¹ç›®æ¦‚è¿°

**ç ”ç©¶æ¥æº**: IJCAI 2023 è®ºæ–‡  
**è®ºæ–‡æ ‡é¢˜**: FedDWA: Personalized Federated Learning with Dynamic Weight Adjustment  
**æ ¸å¿ƒç›®æ ‡**: ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹  (Personalized Federated Learning)  
**ä¸»è¦åˆ›æ–°**: åŠ¨æ€æƒé‡è°ƒæ•´ (Dynamic Weight Adjustment)

### æ”¯æŒçš„è”é‚¦å­¦ä¹ ç®—æ³•

1. **FedDWA** - åŠ¨æ€æƒé‡èšåˆ (æœ¬é¡¹ç›®æ ¸å¿ƒ)
2. **FedAvg** - è”é‚¦å¹³å‡ (åŸºçº¿æ–¹æ³•)
3. **FedProx** - è¿‘ç«¯æ­£åˆ™åŒ–
4. **FedNova** - å½’ä¸€åŒ–å¹³å‡
5. **FedSAM** - é”åº¦æ„ŸçŸ¥æœ€å°åŒ–
6. **MOON** - æ¨¡å‹å¯¹æ¯”å­¦ä¹ 

---

## ğŸ—ï¸ æ•´ä½“ç³»ç»Ÿæ¶æ„

### ä¸‰å±‚æ¶æ„è®¾è®¡

#### 1ï¸âƒ£ Server Layer (æœåŠ¡å™¨å±‚)
- **èŒè´£**: å®¢æˆ·ç«¯é€‰æ‹©ã€æ¨¡å‹èšåˆã€å…¨å±€åè°ƒ
- **æ ¸å¿ƒç±»**:
  - `ServerBase`: åŸºç¡€æœåŠ¡å™¨ç±»ï¼Œå®ç°é€šç”¨åŠŸèƒ½
  - `FedDWA/FedAvg/FedProx/...`: ç®—æ³•ç‰¹å®šå®ç°
- **ä¸»è¦æ–¹æ³•**:
  - `dataset_division()`: æ•°æ®é›†åˆ’åˆ†ä¸Non-IIDåˆ†é…
  - `select_client()`: å®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥
  - `send_models()`: å‘å®¢æˆ·ç«¯åˆ†å‘æ¨¡å‹
  - `receive_models()`: æ¥æ”¶å®¢æˆ·ç«¯æ›´æ–°
  - `aggregated()`: æ¨¡å‹èšåˆ
  - `evaluate_acc()`: è¯„ä¼°å‡†ç¡®ç‡

#### 2ï¸âƒ£ Client Layer (å®¢æˆ·ç«¯å±‚)
- **èŒè´£**: æ‰§è¡Œæœ¬åœ°è®­ç»ƒã€æ¨¡å‹æ›´æ–°
- **æ ¸å¿ƒç±»**:
  - `ClientBase`: åŸºç¡€å®¢æˆ·ç«¯ç±»
  - `ClientFedDWA`: æ”¯æŒä¸¤æ­¥æ¨¡å‹é¢„æµ‹
- **ä¸»è¦æ–¹æ³•**:
  - `train()`: æœ¬åœ°è®­ç»ƒ
  - `receive_models()`: æ¥æ”¶å…¨å±€æ¨¡å‹
  - `test_accuracy()`: æµ‹è¯•å‡†ç¡®ç‡
  - `train_accuracy()`: è®­ç»ƒå‡†ç¡®ç‡

#### 3ï¸âƒ£ Model Layer (æ¨¡å‹å±‚)
- **èŒè´£**: æä¾›å¤šç§ç¥ç»ç½‘ç»œæ¶æ„
- **æ”¯æŒèŒƒå›´**: ä»ç®€å•CNNåˆ°Transformerå…¨è¦†ç›–

---

## ğŸ’¡ FedDWA æ ¸å¿ƒç®—æ³•

### ç®—æ³•æ€æƒ³

**åŠ¨æ€æƒé‡è°ƒæ•´ (Dynamic Weight Adjustment)** çš„æ ¸å¿ƒæ˜¯ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯è®¡ç®—ä¸ªæ€§åŒ–çš„èšåˆæƒé‡ï¼ŒåŸºäºæ¨¡å‹ç›¸ä¼¼åº¦é€‰æ‹©æœ€ç›¸å…³çš„é‚»å±…ã€‚

### ç®—æ³•æµç¨‹

```
For each global round t:
  1. Serveré€‰æ‹©Kä¸ªå®¢æˆ·ç«¯å‚ä¸è®­ç»ƒ
  2. Serverå‘æ¯ä¸ªå®¢æˆ·ç«¯iå‘é€æ¨¡å‹ w_i^{t-1}
  
  3. Client i è¿›è¡Œæœ¬åœ°è®­ç»ƒ:
     - åœ¨æœ¬åœ°æ•°æ®ä¸Šè®­ç»ƒEä¸ªepoch â†’ å¾—åˆ° w_i^t
     - é¢å¤–è®­ç»ƒ1æ­¥ â†’ å¾—åˆ° w_i^{t+1} (next_step_model)
  
  4. Client i ä¸Šä¼  (w_i^t, w_i^{t+1}) åˆ°Server
  
  5. Serverè®¡ç®—æœ€ä¼˜æƒé‡çŸ©é˜µ W:
     W[j,i] âˆ 1 / ||w_i^{t+1} - w_j^t||Â²
     
  6. åˆ—å½’ä¸€åŒ– + Top-Kå‰ªæ:
     - å¯¹æ¯ä¸€åˆ—å½’ä¸€åŒ–ä½¿å¾— Î£_j W[j,i] = 1
     - åªä¿ç•™Top-Kä¸ªæœ€å¤§æƒé‡ï¼Œå…¶ä½™è®¾ä¸º0
     - å†æ¬¡å½’ä¸€åŒ–
  
  7. ä¸ªæ€§åŒ–èšåˆ:
     w_i^{new} = Î£_{jâˆˆTop-K} W[j,i] * w_j^t
  
  8. ä¸‹ä¸€è½®å‘é€ w_i^{new} ç»™å®¢æˆ·ç«¯i
```

### å…³é”®å‚æ•°

- `feddwa_topk`: Top-Ké‚»å±…æ•° (é»˜è®¤5)
- `next_round`: ä¸‹ä¸€æ­¥é¢„æµ‹è½®æ•° (é»˜è®¤1)

### ä¸FedAvgå¯¹æ¯”

| ç‰¹æ€§ | FedAvg | FedDWA |
|------|--------|--------|
| èšåˆæ–¹å¼ | å…¨å±€ç»Ÿä¸€æƒé‡ (æ•°æ®é‡åŠ æƒ) | ä¸ªæ€§åŒ–æƒé‡ (æ¨¡å‹ç›¸ä¼¼åº¦) |
| å‘é€æ¨¡å‹ | æ‰€æœ‰å®¢æˆ·ç«¯æ”¶åˆ°ç›¸åŒæ¨¡å‹ | æ¯ä¸ªå®¢æˆ·ç«¯æ”¶åˆ°ä¸åŒæ¨¡å‹ |
| é€šä¿¡å¼€é”€ | 1Ã— (å•å‘æ¨¡å‹) | 2Ã— (éœ€è¦next_step_model) |
| Non-IIDé€‚åº”æ€§ | ä¸€èˆ¬ | å¼º |

---

## ğŸ§± æ”¯æŒçš„æ¨¡å‹æ¶æ„

### 1. åŸºç¡€å·ç§¯ç¥ç»ç½‘ç»œ

#### CIFAR10Model / CIFAR100Model
```
æ¶æ„:
  Conv2d(3â†’32, k=3, p=1) â†’ BN â†’ ReLU â†’ MaxPool(2Ã—2)
  Conv2d(32â†’64, k=3, p=1) â†’ BN â†’ ReLU â†’ MaxPool(2Ã—2)
  Flatten â†’ FC(2304â†’512) â†’ ReLU â†’ Dropout(0.5)
  FC(512â†’num_classes)

å‚æ•°é‡: ~2.3M
ç‰¹ç‚¹: æ”¯æŒHead/Bodyåˆ†ç¦» (ç”¨äºä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ )
```

#### FedAvgCNN
```
ç»å…¸è”é‚¦å­¦ä¹ åŸºçº¿æ¨¡å‹
Conv2d(in_channelsâ†’32, k=5) â†’ ReLU â†’ MaxPool
Conv2d(32â†’64, k=5) â†’ ReLU â†’ MaxPool
Flatten â†’ FC(dimâ†’512) â†’ ReLU â†’ FC(512â†’num_classes)
```

### 2. æ®‹å·®ç½‘ç»œ

#### ResNet8 / ResNet18
```
æ”¯æŒæ•°æ®é›†: CIFAR-10/100, Tiny-ImageNet, GPR Custom
ç‰¹ç‚¹: 
  - ä½¿ç”¨æ®‹å·®å— (ResBlock)
  - æ”¯æŒé¢„è®­ç»ƒæƒé‡ (zero_init_residual=True)
  - è‡ªé€‚åº”å¹³å‡æ± åŒ– (AdaptiveAvgPool2d)
```

### 3. ç°ä»£é«˜æ•ˆæ¶æ„

#### MobileViT (Vision Transformer for Mobile)
```python
self.model = timm.create_model('mobilevit_s', 
                                pretrained=True, 
                                num_classes=num_classes)

GPRæ¨¡å¼å¢å¼º:
  self.gpr_preprocess = nn.Sequential(
      InstanceNorm2d(3, affine=True),      # å¯å­¦ä¹ çš„ä¿¡å·å½’ä¸€åŒ–
      Conv2d(3â†’16, k=(5,1), p=(2,0)),      # æ—¶é—´åŸŸå¢å¼º(å‚ç›´)
      BatchNorm2d(16) â†’ ReLU,
      Conv2d(16â†’16, k=(1,5), p=(0,2)),     # ç©ºé—´åŸŸå¢å¼º(æ°´å¹³)
      BatchNorm2d(16) â†’ ReLU,
      Conv2d(16â†’3, k=1),                   # èåˆå›3é€šé“
      BatchNorm2d(3)
  )

å‚æ•°é‡: ~5M
ç‰¹ç‚¹: è½»é‡çº§ã€é€‚åˆç§»åŠ¨ç«¯éƒ¨ç½²
```

#### EfficientNet-B0
```python
self.model = timm.create_model('tf_efficientnet_b0', 
                                pretrained=False, 
                                num_classes=num_classes)

å‚æ•°é‡: ~5M
ç‰¹ç‚¹: å¤åˆç¼©æ”¾ç­–ç•¥ä¼˜åŒ–æ·±åº¦/å®½åº¦/åˆ†è¾¨ç‡
```

### 4. å‰æ²¿å¤šæ¨¡æ€æ¶æ„

#### FedCLIP (CLIP for Federated Learning)

**æ ¸å¿ƒæ¶æ„**:
```
1. CLIP Backbone (å†»ç»“, ~87Må‚æ•°):
   â”œâ”€â”€ Image Encoder: ViT-B/32 or ViT-L/14
   â””â”€â”€ Text Encoder: Transformer

2. Trainable Adapter (~0.5Må‚æ•°):
   fea_attn = Sequential(
       MaskedMLP(dim, dim),
       BatchNorm1d(dim),
       ReLU(),
       MaskedMLP(dim, dim),
       Softmax(dim=1)
   )

3. CoOp (Context Optimization) [å¯é€‰]:
   PromptLearner: å­¦ä¹  n_ctx=16 ä¸ªä¸Šä¸‹æ–‡å‘é‡
```

**MaskedMLP è¯¦è§£**:
```python
class MaskedMLP(nn.Module):
    """ç¨€ç–è‡ªé€‚åº”å…¨è¿æ¥å±‚"""
    def __init__(self, in_size, out_size):
        self.weight = nn.Parameter(Tensor(out_size, in_size))
        self.bias = nn.Parameter(Tensor(out_size))
        self.threshold = nn.Parameter(Tensor(out_size))  # å¯å­¦ä¹ é˜ˆå€¼
    
    def mask_generation(self):
        # äºŒå€¼åŒ–: åªä¿ç•™ |weight| > threshold çš„è¿æ¥
        abs_weight = torch.abs(self.weight)
        mask = BinaryStep(abs_weight - self.threshold)
        return mask
    
    def forward(self, x):
        mask = self.mask_generation()
        masked_weight = self.weight * mask
        return F.linear(x, masked_weight, self.bias)
```

**CoOp ç‰©ç†å…ˆéªŒåˆå§‹åŒ–** (GPRä¸“ç”¨):
```python
gpr_init_text = "GPR B-scan signal showing subsurface dielectric reflection"

# å°†ç‰©ç†æè¿°ç¼–ç ä¸º Embedding
tokenized_init = clip.tokenize(gpr_init_text)
embedding = clip_model.token_embedding(tokenized_init)

# ç”¨ç‰©ç†å‘é‡åˆå§‹åŒ–å‰n_initä¸ªä¸Šä¸‹æ–‡
ctx_vectors[:n_init, :] = embedding[0, 1:1+n_init, :]
```

**Prompt Ensemble**:
```python
# è‡ªå®šä¹‰GPRæè¿° (ä¸“å®¶çŸ¥è¯†)
custom_gpr_prompts = {
    "Crack": [
        "GPR B-scan showing a hyperbolic reflection from a crack",
        "discontinuity in subsurface layers indicating a fracture"
    ],
    # ... 8ç±»åˆ«ï¼Œæ¯ç±»3-5ä¸ªæè¿°
}

# é€šç”¨æ¨¡æ¿
templates = [
    "a ground penetrating radar image showing {}",
    "a GPR B-scan of {}",
    # ... 6ä¸ªæ¨¡æ¿
]

# æ··åˆä¸“å®¶çŸ¥è¯†ä¸é€šç”¨æ¨¡æ¿
final_prompts = custom_prompts[class_name] + template_prompts
text_features = clip.encode_text(final_prompts).mean(dim=0)
```

**å‰å‘ä¼ æ’­**:
```python
def forward(self, x):
    # 1. CLIPç¼–ç å›¾åƒ (å†»ç»“)
    image_features = self.model.encode_image(x).float()
    
    # 2. Adapteræ³¨æ„åŠ›
    attn_weights = self.fea_attn(image_features)
    image_features = torch.mul(attn_weights, image_features)
    
    # 3. å½’ä¸€åŒ–
    image_features = F.normalize(image_features, dim=1)
    
    # 4. è®¡ç®—ç›¸ä¼¼åº¦ (Logits)
    text_features = self.get_text_features()  # æ¥è‡ªPromptæˆ–CoOp
    logit_scale = self.model.logit_scale.exp()
    logits = logit_scale * (image_features @ text_features.t())
    
    return logits
```

---

#### GPR-FedSense (Ground Penetrating Radar ä¸“ç”¨)

**ä¸‰å±‚åˆ†ç¦»å¼æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Module 1: æœ¬åœ°ç§æœ‰å±‚ (ä¸å‚ä¸è”é‚¦èšåˆ)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GPRSignalNorm:                                 â”‚
â”‚    - å¯å­¦ä¹ çš„ gamma, beta, gain                â”‚
â”‚    - å®ä¾‹å½’ä¸€åŒ– (é€‚é…è®¾å¤‡å·®å¼‚)                  â”‚
â”‚                                                 â”‚
â”‚  GPRFeatureExtractor:                           â”‚
â”‚    â”œâ”€ Stage1: Conv â†’ BN â†’ ReLU (æµ…å±‚ç‰¹å¾)      â”‚
â”‚    â”œâ”€ TimeConv: (5Ã—1) å‚ç›´å·ç§¯ (æ·±åº¦åå°„)      â”‚
â”‚    â”œâ”€ SpatialConv: (1Ã—5) æ°´å¹³å·ç§¯ (æ¨ªå‘å»¶ç»­)   â”‚
â”‚    â””â”€ Fusion: Concat â†’ Conv1Ã—1                 â”‚
â”‚  è¾“å‡º: 128ç»´ç‰¹å¾                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Module 2: å…¨å±€å…±äº«å±‚ (è”é‚¦èšåˆ)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ”¯æŒä¸‰ç§Backbone:                              â”‚
â”‚    1. CNN: Conv(256) â†’ Conv(512) â†’ AvgPool     â”‚
â”‚    2. ResNet18: æ ‡å‡†ResNet-18 (ä¿®æ”¹ç¬¬ä¸€å±‚)      â”‚
â”‚    3. MobileViT: timm.mobilevitv2_050          â”‚
â”‚  è¾“å‡º: 512ç»´è¯­ä¹‰ç‰¹å¾                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Module 3: ä¸ªæ€§åŒ–åˆ†ç±»å¤´ (ALAè‡ªé€‚åº”èšåˆ)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Dropout(0.2) â†’ FC(512â†’256) â†’ ReLU             â”‚
â”‚  Dropout(0.1) â†’ FC(256â†’8)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**è®¾è®¡ç†å¿µ**:
1. **æœ¬åœ°ç§æœ‰å±‚**: é€‚é…ä¸åŒè®¾å¤‡çš„ä¿¡å·ç‰¹æ€§ (å¢ç›Šã€å™ªå£°ã€é‡‡æ ·ç‡ä¸åŒ)
2. **å…¨å±€å…±äº«å±‚**: å­¦ä¹ é€šç”¨çš„åœ°ä¸‹ç›®æ ‡ç‰¹å¾ (è·¨è®¾å¤‡çŸ¥è¯†å…±äº«)
3. **ä¸ªæ€§åŒ–åˆ†ç±»å¤´**: å¤„ç†Non-IIDæ•°æ®åˆ†å¸ƒ (æ¯ä¸ªåœºåœ°ç±»åˆ«åˆ†å¸ƒä¸åŒ)

**æ—¶ç©ºç‰¹å¾æå–å™¨è¯¦è§£**:
```python
class GPRFeatureExtractor(nn.Module):
    def forward(self, x):
        # ä¿¡å·å½’ä¸€åŒ– (é€‚é…è®¾å¤‡)
        x = self.signal_norm(x)  # [B, 3, H, W]
        
        # æµ…å±‚çº¹ç†ç‰¹å¾
        x = self.stage1(x)  # [B, 64, H, W]
        
        # å¹¶è¡Œæå–æ—¶ç©ºç‰¹å¾
        time_feat = self.time_conv(x)     # [B, 64, H, W] å‚ç›´å·ç§¯
        spatial_feat = self.spatial_conv(x)  # [B, 64, H, W] æ°´å¹³å·ç§¯
        
        # ç‰¹å¾èåˆ
        x = torch.cat([time_feat, spatial_feat], dim=1)  # [B, 128, H, W]
        x = self.fusion(x)  # [B, 128, H, W]
        
        return x
```

**FedDWAæ¥å£å®ç°**:
```python
def get_head_val(self):
    """è·å–åˆ†ç±»å¤´å‚æ•° (ç”¨äºä¸ªæ€§åŒ–èšåˆ)"""
    return [copy.deepcopy(p) for p in self.classifier.parameters()]

def get_body_val(self):
    """è·å–å…±äº«å±‚å‚æ•° (ç”¨äºå…¨å±€èšåˆ)"""
    return [copy.deepcopy(p) for p in self.shared_backbone.parameters()]

def get_local_val(self):
    """è·å–æœ¬åœ°ç§æœ‰å±‚å‚æ•° (ä¸å‚ä¸èšåˆ)"""
    return [copy.deepcopy(p) for p in self.local_extractor.parameters()]
```

---

## ğŸ”„ å®¢æˆ·ç«¯-æœåŠ¡å™¨äº¤äº’æµç¨‹

### å•è½®è®­ç»ƒæµç¨‹

```mermaid
sequenceDiagram
    participant S as Server
    participant C1 as Client 1
    participant C2 as Client 2
    participant CN as Client N
    
    S->>S: 1. select_client()
    Note over S: éšæœºé€‰æ‹© fracÃ—N ä¸ªå®¢æˆ·ç«¯
    
    S->>C1: 2. send_models(w_global)
    S->>C2: 2. send_models(w_global)
    
    C1->>C1: 3. train(E epochs)
    C2->>C2: 3. train(E epochs)
    Note over C1,C2: æœ¬åœ°è®­ç»ƒ<br/>FedDWAé¢å¤–è®¡ç®—next_step
    
    C1->>S: 4. upload(w_1^t, w_1^{t+1})
    C2->>S: 4. upload(w_2^t, w_2^{t+1})
    
    S->>S: 5. aggregated()
    Note over S: FedAvg: w_global = Î£ Î±_i*w_i<br/>FedDWA: w_i^new = Î£ W[j,i]*w_j
    
    S->>S: 6. evaluate_acc()
```

### FedDWAç‰¹æ®Šæµç¨‹

```python
# Clientç«¯
class ClientFedDWA:
    def train(self):
        # Step 1: æ­£å¸¸è®­ç»ƒEä¸ªepoch
        for epoch in range(self.E):
            for batch in self.train_loader:
                loss = self.train_one_step(batch)
        
        # Step 2: ä¿å­˜å½“å‰æ¨¡å‹å‚æ•°
        self.model_params_t = copy.deepcopy(self.model.state_dict())
        
        # Step 3: é¢å¤–è®­ç»ƒnext_roundæ­¥
        for _ in range(self.next_round):
            for batch in self.train_loader:
                loss = self.train_one_step(batch)
        
        # Step 4: ä¿å­˜ä¸‹ä¸€æ­¥æ¨¡å‹å‚æ•°
        self.next_step_model = copy.deepcopy(self.model.state_dict())
        
        # Step 5: æ¢å¤åˆ°tæ—¶åˆ»æ¨¡å‹ (ç”¨äºè¯„ä¼°)
        self.model.load_state_dict(self.model_params_t)
        
        return loss

# Serverç«¯
class FedDWA:
    def cal_optimal_weight(self):
        """è®¡ç®—æœ€ä¼˜æƒé‡çŸ©é˜µ"""
        W = np.zeros([K, K])  # K = len(selected_clients)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        for i in range(K):
            for j in range(K):
                # W[j,i] = 1 / ||w_i^{t+1} - w_j^t||Â²
                diff = self.flatten(next_models[i]) - self.flatten(current_models[j])
                W[j,i] = 1.0 / (torch.norm(diff)**2)
        
        # åˆ—å½’ä¸€åŒ–
        W = self.column_normalization(W)
        
        # Top-Ké€‰æ‹©
        W = self.column_top_k(W, K=self.feddwa_topk)
        
        # å†æ¬¡å½’ä¸€åŒ–
        W = self.column_normalization(W)
        
        return W
    
    def aggregated(self, W):
        """ä¸ªæ€§åŒ–èšåˆ"""
        for i in range(K):
            w_i_new = {}
            for param_name in model_params:
                w_i_new[param_name] = sum(
                    W[j,i] * current_models[j][param_name]
                    for j in range(K)
                )
            self.send_client_models[i] = w_i_new
```

---

## ğŸ“Š æ•°æ®å¤„ç†ä¸Non-IIDè®¾ç½®

### æ”¯æŒçš„æ•°æ®é›†

| æ•°æ®é›† | ç±»åˆ«æ•° | è®­ç»ƒé›† | æµ‹è¯•é›† | å›¾åƒå°ºå¯¸ |
|--------|--------|--------|--------|----------|
| CIFAR-10 | 10 | 50,000 | 10,000 | 32Ã—32Ã—3 |
| CIFAR-100 | 100 | 50,000 | 10,000 | 32Ã—32Ã—3 |
| CINIC-10 | 10 | 90,000 | 90,000 | 32Ã—32Ã—3 |
| Tiny-ImageNet | 200 | 100,000 | 10,000 | 64Ã—64Ã—3 |
| GPR Custom | 8 | è‡ªå®šä¹‰ | è‡ªå®šä¹‰ | 224Ã—224Ã—3 |

### Non-IIDåˆ†å¸ƒç±»å‹

#### Type 8 - ç—…æ€å¼‚æ„ (Pathological Non-IID)
```python
def noniid_type8(dataset, num_users):
    """
    æ¯ä¸ªå®¢æˆ·ç«¯åªæœ‰2ä¸ªç±»åˆ«çš„æ•°æ®
    ä¾‹: Client 0 â†’ [ç±»åˆ«0, ç±»åˆ«1]
        Client 1 â†’ [ç±»åˆ«1, ç±»åˆ«2]
        ...
    æç«¯Non-IIDï¼Œæ¨¡æ‹Ÿæœ€åæƒ…å†µ
    """
    shards_per_user = 2
    num_shards = num_users * shards_per_user
    idx_shard = list(range(num_shards))
    
    # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ†é…shard
    dict_users = {i: np.array([]) for i in range(num_users)}
    for i in range(num_users):
        rand_set = set(np.random.choice(idx_shard, shards_per_user, replace=False))
        idx_shard = list(set(idx_shard) - rand_set)
        dict_users[i] = np.concatenate([shards[idx] for idx in rand_set])
```

#### Type 9 - Dirichletåˆ†å¸ƒ (Practical Non-IID 1)
```python
def noniid_type9(dataset, num_users, num_classes, dirichlet_alpha=0.1):
    """
    ä½¿ç”¨Dirichlet(Î±)åˆ†å¸ƒåˆ†é…æ•°æ®
    Î±è¶Šå°ï¼ŒNon-IIDç¨‹åº¦è¶Šé«˜
    
    Î± = 0.1: é«˜åº¦Non-IID (æ¯ä¸ªå®¢æˆ·ç«¯æ•°æ®ä¸¥é‡åæ–œ)
    Î± = 1.0: ä¸­åº¦Non-IID
    Î± = 10.0: æ¥è¿‘IID
    """
    label_distribution = np.random.dirichlet([dirichlet_alpha]*num_users, num_classes)
    # label_distribution[k,i] = ç±»åˆ«kåœ¨å®¢æˆ·ç«¯içš„æ¯”ä¾‹
    
    for k in range(num_classes):
        idx_k = np.where(labels == k)[0]
        np.random.shuffle(idx_k)
        
        proportions = label_distribution[k]
        # æŒ‰æ¯”ä¾‹åˆ†é…ç±»åˆ«kçš„æ ·æœ¬ç»™å„å®¢æˆ·ç«¯
        proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]
        splits = np.split(idx_k, proportions)
        
        for i, split in enumerate(splits):
            dict_users[i] = np.concatenate([dict_users[i], split])
```

#### Type 10 - ç±»åˆ«æ•°+æ¯”ä¾‹ (Practical Non-IID 2)
```python
def noniid_type10(dataset, num_users, num_types=4, ratio=0.8):
    """
    æ¯ä¸ªå®¢æˆ·ç«¯:
      - ä¸»å¯¼ç±»åˆ«: num_types ä¸ªç±»ï¼Œå æ¯” ratio
      - å…¶ä½™ç±»åˆ«: å¹³å‡åˆ†é…å‰©ä½™çš„ (1-ratio)
    
    ä¾‹: num_types=4, ratio=0.8, num_classes=10
      Client 0: [ç±»0,1,2,3] å 80%, [ç±»4,5,6,7,8,9] å 20%
    
    æ¨¡æ‹ŸçœŸå®åœºæ™¯ (æŸäº›ç±»åˆ«æ›´å¸¸è§)
    """
    main_classes = np.random.choice(num_classes, num_types, replace=False)
    other_classes = set(range(num_classes)) - set(main_classes)
    
    # åˆ†é…80%ç»™ä¸»å¯¼ç±»åˆ«
    main_samples = int(total_samples * ratio / num_types)
    other_samples = int(total_samples * (1-ratio) / len(other_classes))
    
    for c in main_classes:
        dict_users[i] = np.concatenate([dict_users[i], idx_class[c][:main_samples]])
    for c in other_classes:
        dict_users[i] = np.concatenate([dict_users[i], idx_class[c][:other_samples]])
```

---

## ğŸš€ é«˜çº§ä¼˜åŒ–ç­–ç•¥

### 1. FedVLS (Vacant-class Distillation)

**é—®é¢˜**: Non-IIDåœºæ™¯ä¸‹ï¼Œå®¢æˆ·ç«¯å¯èƒ½ç¼ºå¤±æŸäº›ç±»åˆ«çš„æ•°æ®  
**è§£å†³**: ä½¿ç”¨å…¨å±€æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œå¯¹æœ¬åœ°ç¼ºå¤±ç±»åˆ«è¿›è¡ŒçŸ¥è¯†è’¸é¦

```python
def compute_fedvls_loss(student_output, teacher_output, local_labels, num_classes):
    """
    student_output: æœ¬åœ°æ¨¡å‹è¾“å‡º [B, C]
    teacher_output: å…¨å±€æ¨¡å‹è¾“å‡º [B, C]
    local_labels: æœ¬åœ°çœŸå®æ ‡ç­¾ [B]
    """
    # æ‰¾å‡ºæœ¬åœ°å­˜åœ¨çš„ç±»åˆ«
    present_classes = torch.unique(local_labels)
    vacant_classes = [c for c in range(num_classes) if c not in present_classes]
    
    # å¯¹ç©ºç½®ç±»åˆ«è¿›è¡Œè’¸é¦
    distill_loss = 0
    for c in vacant_classes:
        # KLæ•£åº¦: å­¦ç”Ÿåœ¨ç±»åˆ«cä¸Šçš„é¢„æµ‹åˆ†å¸ƒ åº”æ¥è¿‘ æ•™å¸ˆ
        distill_loss += F.kl_div(
            F.log_softmax(student_output[:, c], dim=0),
            F.softmax(teacher_output[:, c], dim=0)
        )
    
    return distill_loss

# æ€»æŸå¤±
loss = ce_loss + fedvls_alpha * fedvls_loss
```

**å‚æ•°**:
- `--use_fedvls`: å¯ç”¨FedVLS
- `--fedvls_alpha`: è’¸é¦æŸå¤±æƒé‡ (é»˜è®¤1.0)

### 2. FedDecorr (Feature Decorrelation)

**é—®é¢˜**: ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§å¯¼è‡´å†—ä½™ï¼Œé™ä½æ¨¡å‹æ³›åŒ–èƒ½åŠ›  
**è§£å†³**: æ·»åŠ å»ç›¸å…³çº¦æŸï¼Œé¼“åŠ±ç‰¹å¾æ­£äº¤

```python
def compute_feddecorr_loss(features):
    """
    features: æ¨¡å‹ä¸­é—´å±‚ç‰¹å¾ [B, D]
    ç›®æ ‡: ä½¿ç‰¹å¾çš„åæ–¹å·®çŸ©é˜µæ¥è¿‘å•ä½çŸ©é˜µ
    """
    # 1. å½’ä¸€åŒ–ç‰¹å¾
    features = F.normalize(features, dim=1)  # [B, D]
    
    # 2. è®¡ç®—åæ–¹å·®çŸ©é˜µ
    cov = features.T @ features / features.size(0)  # [D, D]
    
    # 3. å»ç›¸å…³æŸå¤± (å¸Œæœ›éå¯¹è§’çº¿å…ƒç´ ä¸º0)
    decorr_loss = (cov ** 2).sum() - (cov.diag() ** 2).sum()
    
    return decorr_loss

# æ€»æŸå¤±
loss = ce_loss + feddecorr_beta * decorr_loss
```

**å‚æ•°**:
- `--use_feddecorr`: å¯ç”¨FedDecorr
- `--feddecorr_beta`: å»ç›¸å…³æŸå¤±æƒé‡ (é»˜è®¤0.1)

**é€‚ç”¨åœºæ™¯**: GPR-FedSenseæ¨¡å‹ï¼Œåœ¨`get_features()`è¿”å›çš„ç‰¹å¾ä¸Šåº”ç”¨

### 3. ALA (Adaptive Layer Aggregation)

**é—®é¢˜**: ä¸åŒå±‚å¯¹Non-IIDçš„æ•æ„Ÿåº¦ä¸åŒ (æµ…å±‚æ›´é€šç”¨ï¼Œæ·±å±‚æ›´ä¸ªæ€§åŒ–)  
**è§£å†³**: ä¸ºä¸åŒå±‚å­¦ä¹ è‡ªé€‚åº”èšåˆæƒé‡

```python
class ALA:
    def __init__(self, model, rand_percent=80, layer_idx=0, eta=1.0):
        self.rand_percent = rand_percent
        self.layer_idx = layer_idx
        self.eta = eta  # æƒé‡å­¦ä¹ ç‡
        
        # ä¸ºæ¯ä¸€å±‚åˆå§‹åŒ–æƒé‡
        self.weights = {name: 1.0 for name, _ in model.named_parameters()}
    
    def adaptive_aggregate(self, global_params, local_params, train_loader):
        """
        ä¸ºæ¯ä¸€å±‚å­¦ä¹ æƒé‡ w_l âˆˆ [0,1]
        æ–°å‚æ•° = w_l * global_params + (1 - w_l) * local_params
        """
        # 1. é‡‡æ ·æœ¬åœ°æ•°æ® (rand_percent%)
        sample_data = self.sample_data(train_loader, self.rand_percent)
        
        # 2. å¯¹æ¯ä¸€å±‚è®¡ç®—æ¢¯åº¦
        for name, param in model.named_parameters():
            if self.should_aggregate_layer(name):
                # è®¡ç®—è¯¥å±‚æƒé‡çš„æ¢¯åº¦
                grad = self.compute_weight_gradient(name, sample_data)
                
                # æ›´æ–°æƒé‡ (æ¢¯åº¦ä¸Šå‡)
                self.weights[name] += self.eta * grad
                self.weights[name] = np.clip(self.weights[name], 0, 1)
        
        # 3. åº”ç”¨æƒé‡èšåˆ
        for name, param in model.named_parameters():
            w = self.weights[name]
            param.data = w * global_params[name] + (1 - w) * local_params[name]
```

**å‚æ•°**:
- `--rand_percent`: é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤80%)
- `--layer_idx`: æ§åˆ¶æƒé‡èŒƒå›´ (ä»ç¬¬å‡ å±‚å¼€å§‹èšåˆ)
- `--eta`: æƒé‡å­¦ä¹ ç‡ (é»˜è®¤1.0)

### 4. Learning Rate Decay

**ç­–ç•¥**: éšç€è®­ç»ƒè¿›è¡Œï¼Œé€æ¸é™ä½å­¦ä¹ ç‡

```python
def adjust_learning_rate(optimizer, round_idx, lr_decay, lr_decay_step):
    """
    lr_t = lr_0 * (lr_decay)^{round_idx // lr_decay_step}
    """
    decay_factor = lr_decay ** (round_idx // lr_decay_step)
    new_lr = args.lr * decay_factor
    
    for param_group in optimizer.param_groups:
        param_group['lr'] = new_lr
```

**å‚æ•°**:
- `--lr_decay`: è¡°å‡å› å­ (é»˜è®¤1.0, å³ä¸è¡°å‡)
- `--lr_decay_step`: æ¯Nè½®è¡°å‡ä¸€æ¬¡ (é»˜è®¤10)

---

## ğŸ“ˆ å®éªŒé…ç½®ä¸ç»“æœ

### ä¸»è¦è¶…å‚æ•°

```bash
# å…¨å±€å‚æ•°
--Tg 100                # å…¨å±€é€šä¿¡è½®æ•°
--client_num 20         # å®¢æˆ·ç«¯æ€»æ•°
--client_frac 0.5       # æ¯è½®å‚ä¸æ¯”ä¾‹ (10ä¸ªå®¢æˆ·ç«¯)

# æœ¬åœ°è®­ç»ƒ
--E 1                   # æœ¬åœ°è®­ç»ƒepochæ•°
--B 20                  # æœ¬åœ°batch size
--lr 0.01               # å­¦ä¹ ç‡
--weight_decay 0.0      # L2æ­£åˆ™åŒ–

# FedDWAç‰¹å®š
--feddwa_topk 5         # Top-Ké‚»å±…æ•°
--next_round 1          # ä¸‹ä¸€æ­¥é¢„æµ‹è½®æ•°

# Non-IIDè®¾ç½®
--non_iidtype 9         # ä½¿ç”¨Dirichletåˆ†å¸ƒ
--alpha_dir 0.1         # Dirichletå‚æ•° (è¶Šå°è¶ŠNon-IID)

# å…¶ä»–ç®—æ³•å‚æ•°
--mu 0.01               # FedProxè¿‘ç«¯é¡¹æƒé‡
--sam_rho 0.05          # FedSAMæ‰°åŠ¨åŠå¾„
--moon_mu 5.0           # MOONå¯¹æ¯”æŸå¤±æƒé‡
```

### è¿è¡Œç¤ºä¾‹

```bash
# FedDWA on CIFAR-10
python main.py \
    --alg feddwa \
    --dataset cifar10tpds \
    --model cnn \
    --Tg 100 \
    --client_num 20 \
    --client_frac 0.5 \
    --feddwa_topk 5 \
    --E 1 --B 20 --lr 0.01 \
    --non_iidtype 9 --alpha_dir 0.1

# FedCLIP with CoOp on GPR
python main.py \
    --alg feddwa \
    --dataset gpr_custom \
    --model fedclip \
    --use_coop --n_ctx 16 \
    --Tg 100 \
    --lr 0.001

# GPR-FedSense with FedVLS + FedDecorr
python main.py \
    --alg feddwa \
    --dataset gpr_custom \
    --model gpr_fed \
    --gpr_backbone resnet18 \
    --use_fedvls --fedvls_alpha 1.0 \
    --use_feddecorr --feddecorr_beta 0.1 \
    --Tg 100
```

### ç»“æœä¿å­˜

```
logs_feddwa/
â”œâ”€â”€ cifar10tpds_feddwa_model=cnn_..._12345.json     # æµ‹è¯•å‡†ç¡®ç‡ã€è®­ç»ƒæŸå¤±
â”œâ”€â”€ cifar10tpds_feddwa_model=cnn_..._results.csv    # è¯¦ç»†é€è½®ç»“æœ
â”œâ”€â”€ cifar10tpds_feddwa_model=cnn_..._model_structure.json
â””â”€â”€ client_confusion_matrices/
    â”œâ”€â”€ confusion_matrix_client_0.png
    â”œâ”€â”€ confusion_matrix_client_1.png
    â””â”€â”€ ...
```

**CSVæ ¼å¼**:
```
Round, Global_Train_Acc, Weighted_Mean_Acc, Round_Duration, Learning_Rate, Client_0_Test_Acc, Client_1_Test_Acc, ...
1, 0.4523, 0.4234, 12.34, 0.01, 0.45, 0.42, ...
2, 0.5123, 0.4934, 11.98, 0.01, 0.52, 0.49, ...
...
```

---

## ğŸŒ åº”ç”¨åœºæ™¯

### 1. åŒ»ç–—å¥åº· (Healthcare)
- **åœºæ™¯**: å¤šå®¶åŒ»é™¢ååŒè®­ç»ƒç–¾ç—…è¯Šæ–­æ¨¡å‹
- **æŒ‘æˆ˜**: æ‚£è€…éšç§ã€åŒ»é™¢é—´æ•°æ®åˆ†å¸ƒå·®å¼‚å¤§
- **è§£å†³**: 
  - FedDWA: ä¸ºæ¯å®¶åŒ»é™¢æä¾›ä¸ªæ€§åŒ–æ¨¡å‹
  - FedCLIP: åŒ»å­¦å›¾åƒ+æŠ¥å‘Šè”åˆå­¦ä¹ 
  - FedVLS: å¤„ç†ç½•è§ç—… (æŸäº›åŒ»é™¢ç¼ºå¤±æ•°æ®)

### 2. æ™ºèƒ½äº¤é€š (Intelligent Transportation)
- **åœºæ™¯**: è‡ªåŠ¨é©¾é©¶è½¦è¾†é—´ååŒæ„ŸçŸ¥
- **æŒ‘æˆ˜**: ä¸åŒåœ°åŒºå¤©æ°”/è·¯å†µå·®å¼‚ã€è®¾å¤‡å¼‚æ„
- **è§£å†³**:
  - GPR-FedSense: è·¯é¢ç¼ºé™·æ£€æµ‹ (æ¢åœ°é›·è¾¾)
  - æœ¬åœ°ç§æœ‰å±‚: é€‚é…ä¸åŒè½¦è½½è®¾å¤‡

### 3. é‡‘èé£æ§ (Finance)
- **åœºæ™¯**: é“¶è¡Œé—´åä½œåæ¬ºè¯ˆæ¨¡å‹è®­ç»ƒ
- **æŒ‘æˆ˜**: å®¢æˆ·éšç§ã€æ¬ºè¯ˆæ¨¡å¼åœ°åŒºå·®å¼‚
- **è§£å†³**:
  - FedProx: è¿‘ç«¯æ­£åˆ™åŒ–é˜²æ­¢æ¨¡å‹åç§»
  - FedDWA: ä¸ºæ¯å®¶é“¶è¡Œå®šåˆ¶é£æ§ç­–ç•¥

### 4. å·¥ä¸šæ£€æµ‹ (Industrial Inspection)
- **åœºæ™¯**: æ¢åœ°é›·è¾¾ã€æ— æŸæ£€æµ‹è®¾å¤‡é—´çŸ¥è¯†å…±äº«
- **æŒ‘æˆ˜**: è®¾å¤‡å‚å•†ä¸åŒã€ä¿¡å·ç‰¹æ€§å·®å¼‚å¤§
- **è§£å†³**:
  - GPR-FedSense: ä¸‰å±‚æ¶æ„ (æœ¬åœ°+å…±äº«+ä¸ªæ€§åŒ–)
  - ä¿¡å·å½’ä¸€åŒ–: é€‚é…ä¸åŒè®¾å¤‡

---

## ğŸ”® æœªæ¥å·¥ä½œæ–¹å‘

### 1. éšç§ä¿æŠ¤å¢å¼º
- **å·®åˆ†éšç§ (Differential Privacy)**:
  ```python
  # åœ¨æ¢¯åº¦ä¸­æ·»åŠ é«˜æ–¯å™ªå£°
  gradient += torch.randn_like(gradient) * noise_scale
  ```
- **åŒæ€åŠ å¯† (Homomorphic Encryption)**: å…è®¸åœ¨åŠ å¯†æ•°æ®ä¸Šç›´æ¥è®¡ç®—
- **å®‰å…¨å¤šæ–¹è®¡ç®— (SMPC)**: å¤šæ–¹è”åˆè®¡ç®—è€Œä¸æ³„éœ²åŸå§‹æ•°æ®

### 2. æ¨¡å‹å‹ç¼©
- **çŸ¥è¯†è’¸é¦ (Knowledge Distillation)**: è®­ç»ƒå°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹
- **æ¨¡å‹å‰ªæ (Pruning)**: ç§»é™¤å†—ä½™å‚æ•° (å·²å®ç°MaskedMLP)
- **é‡åŒ– (Quantization)**: é™ä½å‚æ•°ç²¾åº¦ (FP32 â†’ INT8)

### 3. é€šä¿¡ä¼˜åŒ–
- **æ¢¯åº¦å‹ç¼© (Gradient Compression)**:
  ```python
  # Top-K Sparsification
  k = int(0.1 * gradient.numel())
  topk_values, topk_indices = torch.topk(gradient.abs().flatten(), k)
  compressed_gradient = torch.sparse_coo_tensor(topk_indices, topk_values)
  ```
- **éƒ¨åˆ†å‚æ•°æ›´æ–°**: åªä¸Šä¼ /ä¸‹è½½å˜åŒ–çš„å±‚
- **Over-the-Air Computation**: åˆ©ç”¨æ— çº¿ä¿¡é“ç‰¹æ€§ç›´æ¥èšåˆ

### 4. å¼‚æ­¥è”é‚¦å­¦ä¹ 
- **é—®é¢˜**: åŒæ­¥FLéœ€ç­‰å¾…æœ€æ…¢çš„å®¢æˆ·ç«¯ (straggler problem)
- **è§£å†³**: å¼‚æ­¥èšåˆ + æ—¶é—´æˆ³æ ¡æ­£
  ```python
  # å¼‚æ­¥FedAvg
  def async_aggregate(self, new_model, timestamp):
      staleness = current_time - timestamp
      weight = 1.0 / (1 + staleness)  # è¶Šæ—§æƒé‡è¶Šå°
      self.global_model = weight * new_model + (1-weight) * self.global_model
  ```

### 5. å¤šä»»åŠ¡è”é‚¦å­¦ä¹ 
- åŒæ—¶è®­ç»ƒåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰å¤šä¸ªä»»åŠ¡
- å…±äº«åº•å±‚è¡¨ç¤ºï¼Œä»»åŠ¡ç‰¹å®šå¤´

### 6. è·¨åŸŸè”é‚¦å­¦ä¹ 
- **åŸŸé€‚åº” (Domain Adaptation)**: å¤„ç†ä¸åŒæ•°æ®åˆ†å¸ƒ
- **è¿ç§»å­¦ä¹  (Transfer Learning)**: ä»ä¸€ä¸ªä»»åŠ¡è¿ç§»åˆ°å¦ä¸€ä¸ªä»»åŠ¡

---

## ğŸ“š æŠ€æœ¯æ ˆ (Tech Stack)

### æ·±åº¦å­¦ä¹ æ¡†æ¶
- **PyTorch** 2.0+: æ ¸å¿ƒè®­ç»ƒæ¡†æ¶
- **torchvision**: è§†è§‰æ¨¡å‹ä¸æ•°æ®é›†
- **timm** (PyTorch Image Models): é¢„è®­ç»ƒæ¨¡å‹åº“
- **CLIP** (OpenAI): å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹

### ç§‘å­¦è®¡ç®—
- **NumPy**: æ•°å€¼è®¡ç®—
- **scikit-learn**: è¯„ä¼°æŒ‡æ ‡ (æ··æ·†çŸ©é˜µã€å‡†ç¡®ç‡)

### å¯è§†åŒ–
- **matplotlib**: åŸºç¡€ç»˜å›¾
- **seaborn**: ç»Ÿè®¡å¯è§†åŒ– (æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾)

### æ•°æ®å¤„ç†
- **h5py**: HDF5æ•°æ®æ ¼å¼
- **Pillow**: å›¾åƒå¤„ç†

### å…¶ä»–
- **pathlib**: è·¯å¾„ç®¡ç†
- **argparse**: å‘½ä»¤è¡Œå‚æ•°è§£æ
- **logging**: æ—¥å¿—è®°å½•

---

## ğŸ’¡ å…³é”®åˆ›æ–°ç‚¹æ€»ç»“

### 1ï¸âƒ£ FedDWAç®—æ³•
- **åŠ¨æ€æƒé‡èšåˆ**: åŸºäºæ¨¡å‹ç›¸ä¼¼åº¦è®¡ç®—ä¸ªæ€§åŒ–æƒé‡
- **Top-Kæœºåˆ¶**: åªä¿ç•™æœ€ç›¸å…³çš„Kä¸ªé‚»å±…ï¼Œæé«˜èšåˆæ•ˆç‡
- **ä¸¤æ­¥è®­ç»ƒ**: é€šè¿‡next_step_modelé¢„æµ‹æœªæ¥çŠ¶æ€ï¼Œæ›´å¥½åœ°é€‰æ‹©é‚»å±…

### 2ï¸âƒ£ å¤šæ¨¡æ€è”é‚¦å­¦ä¹  (FedCLIP)
- **é¦–æ¬¡å°†CLIPå¼•å…¥è”é‚¦å­¦ä¹ **: åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€å¯¹é½
- **CoOpç‰©ç†å…ˆéªŒåˆå§‹åŒ–**: ç”¨é¢†åŸŸçŸ¥è¯† (GPR B-scan...) åˆå§‹åŒ–ä¸Šä¸‹æ–‡
- **MaskedMLPç¨€ç–é€‚é…**: åŠ¨æ€ç”Ÿæˆæ©ç ï¼Œå‡å°‘å‚æ•°é‡å’Œè¿‡æ‹Ÿåˆ
- **Prompt Ensemble**: æ··åˆä¸“å®¶çŸ¥è¯†ä¸é€šç”¨æ¨¡æ¿ï¼Œæå‡é²æ£’æ€§

### 3ï¸âƒ£ ä¸“ç”¨é¢†åŸŸé€‚é… (GPR-FedSense)
- **ä¸‰å±‚åˆ†ç¦»æ¶æ„**: æœ¬åœ°ç§æœ‰å±‚ + å…¨å±€å…±äº«å±‚ + ä¸ªæ€§åŒ–å¤´
- **æ—¶ç©ºç‰¹å¾æå–**: å¹¶è¡Œçš„æ—¶é—´åŸŸ (5Ã—1) å’Œç©ºé—´åŸŸ (1Ã—5) å·ç§¯
- **ä¿¡å·å½’ä¸€åŒ–**: å¯å­¦ä¹ çš„gamma/beta/gainï¼Œé€‚é…ä¸åŒè®¾å¤‡
- **æ”¯æŒFedVLSå’ŒFedDecorr**: å¤„ç†ç±»åˆ«ç¼ºå¤±å’Œç‰¹å¾å†—ä½™

### 4ï¸âƒ£ å…¨é¢çš„ç®—æ³•å¯¹æ¯”æ¡†æ¶
- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰ç®—æ³•å…±äº«ServerBaseå’ŒClientBase
- **6ç§ç®—æ³•**: FedDWA, FedAvg, FedProx, FedNova, FedSAM, MOON
- **æ˜“äºæ‰©å±•**: åªéœ€ç»§æ‰¿Baseç±»å¹¶å®ç°`aggregated()`æ–¹æ³•

### 5ï¸âƒ£ ä¸¥è°¨çš„å®éªŒè®¾è®¡
- **3ç§Non-IIDç±»å‹**: Pathological, Dirichlet, ç±»åˆ«æ•°+æ¯”ä¾‹
- **å®Œå–„çš„è¯„ä¼°**: æµ‹è¯•å‡†ç¡®ç‡ã€è®­ç»ƒæŸå¤±ã€æ··æ·†çŸ©é˜µã€è®­ç»ƒæ›²çº¿
- **è‡ªåŠ¨åŒ–**: ä¸€é”®è¿è¡Œ + è‡ªåŠ¨ä¿å­˜ç»“æœ + å¯è§†åŒ–

---

## ğŸ“Š æ¨¡å‹å¤æ‚åº¦å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | FLOPs | è¾“å…¥å°ºå¯¸ | é€‚ç”¨åœºæ™¯ |
|------|--------|-------|----------|----------|
| CIFAR10Model | 2.3M | ~0.5G | 32Ã—32 | è½»é‡çº§åˆ†ç±» |
| FedAvgCNN | 1.2M | ~0.3G | 32Ã—32 | åŸºçº¿å¯¹æ¯” |
| ResNet18 | 11M | ~1.8G | 224Ã—224 | é€šç”¨è§†è§‰ä»»åŠ¡ |
| MobileViT-S | 5M | ~2.0G | 224Ã—224 | ç§»åŠ¨ç«¯éƒ¨ç½² |
| EfficientNet-B0 | 5M | ~0.4G | 224Ã—224 | é«˜æ•ˆæ¨ç† |
| FedCLIP (ViT-B/32) | 87M (å†»ç»“) + 0.5M (å¯è®­ç»ƒ) | ~4.4G | 224Ã—224 | å¤šæ¨¡æ€å­¦ä¹  |
| GPR-FedSense (CNN) | 3M | ~0.8G | 224Ã—224 | æ¢åœ°é›·è¾¾ |
| GPR-FedSense (ResNet18) | 12M | ~2.2G | 224Ã—224 | æ¢åœ°é›·è¾¾ (é«˜ç²¾åº¦) |

**æ³¨**:
- FLOPs åŸºäºå•å¼ å›¾åƒå‰å‘ä¼ æ’­è®¡ç®—
- FedCLIPçš„87Må‚æ•°æ¥è‡ªCLIP backbone (å†»ç»“)ï¼Œå®é™…è®­ç»ƒåªéœ€è¦0.5Må‚æ•°
- GPR-FedSenseçš„å‚æ•°é‡å–å†³äºé€‰æ‹©çš„backbone

---

## ğŸ“ ä»£ç ç»„ç»‡ç»“æ„

```
FedDWA/
â”œâ”€â”€ main.py                      # ä¸»å…¥å£ï¼Œå‚æ•°è§£æ
â”œâ”€â”€ requirements.txt             # ä¾èµ–åº“
â”œâ”€â”€ script.sh                    # æ‰¹é‡å®éªŒè„šæœ¬
â”œâ”€â”€ readme.md                    # é¡¹ç›®è¯´æ˜
â”‚
â”œâ”€â”€ servers/                     # æœåŠ¡å™¨å®ç°
â”‚   â”œâ”€â”€ serverBase.py            # åŸºç¡€æœåŠ¡å™¨ç±»
â”‚   â”œâ”€â”€ serverFedDWA.py          # FedDWAæœåŠ¡å™¨
â”‚   â”œâ”€â”€ serverFedAvg.py          # FedAvgæœåŠ¡å™¨
â”‚   â”œâ”€â”€ serverFedProx.py         # FedProxæœåŠ¡å™¨
â”‚   â”œâ”€â”€ serverFedNova.py         # FedNovaæœåŠ¡å™¨
â”‚   â”œâ”€â”€ serverFedSAM.py          # FedSAMæœåŠ¡å™¨
â”‚   â””â”€â”€ serverMOON.py            # MOONæœåŠ¡å™¨
â”‚
â”œâ”€â”€ clients/                     # å®¢æˆ·ç«¯å®ç°
â”‚   â”œâ”€â”€ clientBase.py            # åŸºç¡€å®¢æˆ·ç«¯ç±»
â”‚   â”œâ”€â”€ clientFedDWA.py          # FedDWAå®¢æˆ·ç«¯ (ä¸¤æ­¥è®­ç»ƒ)
â”‚   â”œâ”€â”€ clientFedAvg.py          # FedAvgå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ clientFedProx.py         # FedProxå®¢æˆ·ç«¯ (è¿‘ç«¯é¡¹)
â”‚   â”œâ”€â”€ clientFedNova.py         # FedNovaå®¢æˆ·ç«¯ (å½’ä¸€åŒ–)
â”‚   â”œâ”€â”€ clientFedSAM.py          # FedSAMå®¢æˆ·ç«¯ (é”åº¦æ„ŸçŸ¥)
â”‚   â””â”€â”€ clientMOON.py            # MOONå®¢æˆ·ç«¯ (å¯¹æ¯”å­¦ä¹ )
â”‚
â”œâ”€â”€ model/                       # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ MLModel.py               # æ‰€æœ‰æ¨¡å‹å®šä¹‰ (1746è¡Œ)
â”‚   â”‚   â”œâ”€â”€ åŸºç¡€æ¨¡å‹ (CNN, MLP, ResNet)
â”‚   â”‚   â”œâ”€â”€ MobileViT (Vision Transformer)
â”‚   â”‚   â”œâ”€â”€ FedCLIP (å¤šæ¨¡æ€)
â”‚   â”‚   â””â”€â”€ GPR-FedSense (æ¢åœ°é›·è¾¾)
â”‚   â””â”€â”€ myresnet.py              # ResNetå˜ä½“
â”‚
â”œâ”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ data_utils.py            # æ•°æ®åŠ è½½ä¸åˆ’åˆ†
â”‚   â”œâ”€â”€ dataset.py               # è‡ªå®šä¹‰æ•°æ®é›†ç±»
â”‚   â”œâ”€â”€ logger.py                # æ—¥å¿—è®°å½•
â”‚   â””â”€â”€ plot_utils.py            # å¯è§†åŒ–å·¥å…·
â”‚
â”œâ”€â”€ logs_feddwa/                 # å®éªŒæ—¥å¿— (è‡ªåŠ¨ç”Ÿæˆ)
â”‚   â”œâ”€â”€ *.json                   # æµ‹è¯•å‡†ç¡®ç‡ã€è®­ç»ƒæŸå¤±
â”‚   â”œâ”€â”€ *.csv                    # è¯¦ç»†é€è½®ç»“æœ
â”‚   â”œâ”€â”€ *_model_structure.json   # æ¨¡å‹ç»“æ„
â”‚   â””â”€â”€ client_confusion_matrices/  # æ··æ·†çŸ©é˜µ
â”‚
â””â”€â”€ data/                        # æ•°æ®é›†ç›®å½• (éœ€ä¸‹è½½)
    â”œâ”€â”€ cifar-10-batches-py/
    â”œâ”€â”€ cifar-100-python/
    â””â”€â”€ gpr_custom/
```

---

## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨æœ¬é¡¹ç›®çš„ä»£ç æˆ–æ€æƒ³ï¼Œè¯·å¼•ç”¨åŸå§‹è®ºæ–‡:

```bibtex
@inproceedings{liu2023feddwa,
  title={FedDWA: Personalized Federated Learning with Dynamic Weight Adjustment},
  author={Liu, Jiahao and Wu, Jiang and Chen, Jinyu and Hu, Miao and Zhou, Yipeng and Wu, Di},
  booktitle={Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)},
  pages={3980--3988},
  year={2023}
}
```

---

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»:
- ğŸ“§ Email: [è¯·å‚è€ƒåŸå§‹ä»“åº“]
- ğŸ› Issues: [GitHub Issues]
- ğŸ“ è®¨è®º: [GitHub Discussions]

---

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE) å¼€æºåè®®ã€‚

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ

**ç‰ˆæœ¬**: v1.0

**ç»´æŠ¤çŠ¶æ€**: ç§¯æç»´æŠ¤ä¸­ âœ…
