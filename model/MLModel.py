# Several basic machine learning models
import torch
from torch import nn
from torch.nn import functional as F
import torchvision
import copy
from torchvision import models
import timm

class LogisticRegression(nn.Module):
    """A simple implementation of Logistic regression model"""

    def __init__(self, num_feature, output_size):
        super(LogisticRegression, self).__init__()

        self.num_feature = num_feature
        self.output_size = output_size
        self.linear = nn.Linear(self.num_feature, self.output_size)

    def forward(self, x):
        x = torch.flatten(x, 1)
        return self.linear(x)


class MLP(nn.Module):
    """A simple implementation of Deep Neural Network model"""

    def __init__(self, num_feature, output_size):
        super(MLP, self).__init__()
        self.hidden = 200
        self.model = nn.Sequential(
            nn.Linear(num_feature, self.hidden),
            nn.Dropout(0.2),
            nn.ReLU(),
            nn.Linear(self.hidden, output_size))

    def forward(self, x):
        return self.model(x)


class MlpModel(nn.Module):
    """
    2-hidden-layer fully connected model, 2 hidden layers with 200 units and a
    BN layer. Categorical Cross Entropy loss.
    """
    def __init__(self, in_features=784, num_classes=10, hidden_dim=200):
        """
        Returns a new MNISTModelBN.
        """
        super(MlpModel, self).__init__()
        self.in_features = in_features
        self.fc0 = torch.nn.Linear(in_features, hidden_dim)
        self.relu0 = torch.nn.ReLU()
        self.fc1 = torch.nn.Linear(hidden_dim, 200)
        self.relu1 = torch.nn.ReLU()
        self.out = torch.nn.Linear(200, num_classes)
        self.bn0 = torch.nn.BatchNorm1d(hidden_dim)
        self.bn_layers = [self.bn0]

    def forward(self, x):
        """
        Returns outputs of model given data x.

        Args:
            - x: (torch.tensor) must be on same device as model

        Returns:
            torch.tensor model outputs, shape (batch_size, 10)
        """
        x = x.reshape(-1, self.in_features)
        a = self.bn0(self.relu0(self.fc0(x)))
        b = self.relu1(self.fc1(a))

        return self.out(b)


class MnistCNN(nn.Module):
    """from fy"""
    def __init__(self, data_in, data_out):
        super(MnistCNN, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=5, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=5, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2))
        self.fc = nn.Linear(7 * 7 * 32, 10)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# https://github.com/katsura-jp/fedavg.pytorch/blob/master/src/models/mlp.py
class FedAvgMLP(nn.Module):
    def __init__(self, in_features=784, num_classes=10, hidden_dim=200):
        super().__init__()
        self.fc1 = nn.Linear(in_features, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, num_classes)
        self.act = nn.ReLU(inplace=True)

    def forward(self, x):
        if x.ndim == 4:
            x = x.view(x.size(0), -1)
        x = self.act(self.fc1(x))
        x = self.fc2(x)
        return x


# https://github.com/katsura-jp/fedavg.pytorch/blob/master/src/models/cnn.py
class FedAvgCNN(nn.Module):
    def __init__(self, in_features=1, num_classes=10, dim=1024):
        super().__init__()
        self.conv1 = nn.Conv2d(in_features,
                               32,
                               kernel_size=5,
                               padding=0,
                               stride=1,
                               bias=True)
        self.conv2 = nn.Conv2d(32,
                               64,
                               kernel_size=5,
                               padding=0,
                               stride=1,
                               bias=True)
        self.fc1 = nn.Linear(dim, 512)
        self.fc = nn.Linear(512, num_classes)

        self.act = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2))

    def forward(self, x):
        x = self.act(self.conv1(x))
        x = self.maxpool(x)
        x = self.act(self.conv2(x))
        x = self.maxpool(x)
        x = torch.flatten(x, 1)
        x = self.act(self.fc1(x))
        x = self.fc(x)
        return x


"""from fy"""
class CifarCNN(nn.Module):
    def __init__(self, data_in, data_out):
        super(CifarCNN, self).__init__()

        self.layer1 = nn.Sequential(
            nn.Conv2d(3, 64, 5),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, 2)
        )
        self.layer2 = nn.Sequential(
            nn.Conv2d(64, 64, 5),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(3, 2)
        )
        self.fc = nn.Sequential(
            nn.Linear(64 * 4 * 4, 384),
            nn.ReLU(),
            nn.Linear(384, 192),
            nn.ReLU(),
            nn.Linear(192, 10),
            nn.LogSoftmax(dim=1)
        )

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        # x = x.view(-1, 64 * 4 * 4)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x


class CifarCNN_MTFL(nn.Module):
    """
    cifar10 model of MTFL
    """

    def __init__(self, data_in, data_out):
        super(CifarCNN_MTFL, self).__init__()

        self.conv0 = torch.nn.Conv2d(3, 32, 3, 1)
        self.relu0 = torch.nn.ReLU()
        self.pool0 = torch.nn.MaxPool2d(2, 2)

        self.conv1 = torch.nn.Conv2d(32, 64, 3, 1)
        self.relu1 = torch.nn.ReLU()
        self.pool1 = torch.nn.MaxPool2d(2, 2)

        self.flat = torch.nn.Flatten()
        self.fc0 = torch.nn.Linear(2304, 512)
        self.relu2 = torch.nn.ReLU()

        self.out = torch.nn.Linear(512, 10)

        self.bn0 = torch.nn.BatchNorm2d(32)
        self.bn1 = torch.nn.BatchNorm2d(64)

        # self.bn_layers = [self.bn0, self.bn1]

    def forward(self, x):
        """
        Returns outputs of model given data x.
        Args:
            - x: (torch.tensor) must be on same device as model
        Returns:
            torch.tensor model outputs, shape (batch_size, 10)
        """
        a = self.bn0(self.pool0(self.relu0(self.conv0(x))))
        b = self.bn1(self.pool1(self.relu1(self.conv1(a))))
        c = self.relu2(self.fc0(self.flat(b)))

        return self.out(c)


def weight_init(m):
    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            torch.nn.init.zeros_(m.bias)


class BasicCNN(nn.Module):
    def __init__(self, data_in, data_out):
        super(BasicCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 5)
        self.fc = nn.Sequential(
            nn.Linear(64 * 5 * 5, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )
        self.apply(weight_init)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 5 * 5)
        x = self.fc(x)
        return x

"""Cluster FL"""
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 6, 5)
        self.pool = torch.nn.MaxPool2d(2, 2)
        self.conv2 = torch.nn.Conv2d(6, 16, 5)
        self.fc1 = torch.nn.Linear(16 * 4 * 4, 62)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 4 * 4)
        x = self.fc1(x)
        return x


"""FedFomo"""
class BaseConvNet(nn.Module):
    def __init__(self, in_features=1, num_classes=10, ):
        super(BaseConvNet, self).__init__()
        self.conv1 = nn.Conv2d(in_features, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


"""
Communication-Efficient Learning of Deep Networks from Decentralized Data
https://github.com/AshwinRJ/Federated-Learning-PyTorch/blob/master/src/models.py
"""
class CNNMnist(nn.Module):
    def __init__(self, data_in, data_out):
        super(CNNMnist, self).__init__()
        self.conv1 = nn.Conv2d(data_in, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, data_out)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)



"""
Communication-Efficient Learning of Deep Networks from Decentralized Data
https://github.com/AshwinRJ/Federated-Learning-PyTorch/blob/master/src/models.py
"""
class CNNFashion_Mnist(nn.Module):
    def __init__(self, data_in, data_out):
        super(CNNFashion_Mnist, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=5, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=5, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2))
        self.fc = nn.Linear(7*7*32, 10)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out


"""
Communication-Efficient Learning of Deep Networks from Decentralized Data
https://github.com/AshwinRJ/Federated-Learning-PyTorch/blob/master/src/models.py
"""
class CNNCifar(nn.Module):
    def __init__(self, data_in, data_out):
        super(CNNCifar, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, data_out)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.log_softmax(x, dim=1)

# TPDS MTFL model
class CIFAR10Model(nn.Module):
    def __init__(self, in_features, num_classes):
        super(CIFAR10Model, self).__init__()
        self.conv0 = torch.nn.Conv2d(3, 32, 3, 1)
        self.relu0 = torch.nn.ReLU()
        self.pool0 = torch.nn.MaxPool2d(2, 2)

        self.conv1 = torch.nn.Conv2d(32, 64, 3, 1)
        self.relu1 = torch.nn.ReLU()
        self.pool1 = torch.nn.MaxPool2d(2, 2)

        self.flat = torch.nn.Flatten()
        self.fc0 = torch.nn.Linear(2304, 512)
        self.relu2 = torch.nn.ReLU()

        self.out = torch.nn.Linear(512, num_classes)

        self.drop = torch.nn.Dropout(p=0.5)

        self.bn0 = torch.nn.BatchNorm2d(32)
        self.bn1 = torch.nn.BatchNorm2d(64)

        self.head = [self.out]
        self.body = [self.conv0,self.conv1,self.bn0, self.bn1,self.fc0]


        # self.bn_layers = [self.bn0, self.bn1]
        self.classifier_layer = [self.fc0, self.out]

    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.head:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals
    
    def get_body_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.body:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals

    def set_head_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.head:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def set_body_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.body:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2


    def forward(self, x):
        a = self.bn0(self.pool0(self.relu0(self.conv0(x))))
        b = self.bn1(self.pool1(self.relu1(self.conv1(a))))
        c = self.relu2(self.drop(self.fc0(self.flat(b))))
        return self.out(c)

# TPDS MTFL model
class CIFAR100Model(nn.Module):
    def __init__(self, in_features, num_classes):
        super(CIFAR100Model, self).__init__()
        self.conv0 = torch.nn.Conv2d(3, 32, 3, 1)
        self.relu0 = torch.nn.ReLU()
        self.pool0 = torch.nn.MaxPool2d(2, 2)

        self.conv1 = torch.nn.Conv2d(32, 64, 3, 1)
        self.relu1 = torch.nn.ReLU()
        self.pool1 = torch.nn.MaxPool2d(2, 2)

        self.flat = torch.nn.Flatten()
        self.fc0 = torch.nn.Linear(2304, 512)
        self.relu2 = torch.nn.ReLU()

        self.out = torch.nn.Linear(512, 100)

        self.drop = torch.nn.Dropout(p=0.5)

        self.bn0 = torch.nn.BatchNorm2d(32)
        self.bn1 = torch.nn.BatchNorm2d(64)

        # self.bn_layers = [self.bn0, self.bn1]
        self.classifier_layer = [self.fc0, self.out]
        self.head = [self.out]
        self.body = [self.conv0,self.conv1,self.bn0, self.bn1,self.fc0]

    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.head:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals
    
    def get_body_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.body:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals

    def set_head_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.head:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def set_body_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.body:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def forward(self, x):
        a = self.bn0(self.pool0(self.relu0(self.conv0(x))))
        b = self.bn1(self.pool1(self.relu1(self.conv1(a))))
        c = self.relu2(self.drop(self.fc0(self.flat(b))))
        return self.out(c)



# from TPDS
class FashionMNISTModel(nn.Module):
    def __init__(self, num_classes):
        """
        Returns a new FashionMNISTModel.

        Args:
            - device: (torch.device) to place model on
        """
        super(FashionMNISTModel, self).__init__()
        self.conv0 = torch.nn.Conv2d(1, 32, 7, padding=3)
        self.act = torch.nn.ReLU()
        self.pool = torch.nn.MaxPool2d(2, 2)
        self.bn0 = torch.nn.BatchNorm2d(32)
        self.conv1 = torch.nn.Conv2d(32, 64, 3, padding=1)
        self.bn1 = torch.nn.BatchNorm2d(64)
        self.out = torch.nn.Linear(64 * 7 * 7, num_classes)
        self.bn_layers = [self.bn0, self.bn1]
        self.head = [self.out]
        self.body = [self.conv0,self.bn0,self.conv1,self.bn1]

    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.head:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals
    
    def get_body_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.body:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals

    def set_head_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.head:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def set_body_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.body:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def forward(self, x):
        """
        Returns outputs of model given data x.

        Args:
            - x: (torch.tensor) must be on same device as model

        Returns:
            torch.tensor model outputs, shape (batch_size, 10)
        """
        x = x.reshape(-1, 1, 28, 28)
        x = self.bn0(self.pool(self.act(self.conv0(x))))
        x = self.bn1(self.pool(self.act(self.conv1(x))))
        x = x.flatten(1)
        return self.out(x)


class FemnistCNN(nn.Module):
    """
    Implements a model with two convolutional layers followed by pooling, and a final dense layer with 2048 units.
    Same architecture used for FEMNIST in "LEAF: A Benchmark for Federated Settings"__
    We use `zero`-padding instead of  `same`-padding used in
     https://github.com/TalwalkarLab/leaf/blob/master/models/femnist/cnn.py.
    """
    def __init__(self, num_classes):
        super(FemnistCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 5)
        self.relu = torch.nn.ReLU()
        self.fc1 = nn.Linear(64 * 4 * 4, 2048)
        self.output = nn.Linear(2048, num_classes)
        self.classifier_layer = [self.fc1, self.output]
        self.head = [self.output]
        self.body = [self.conv1,self.conv2,self.fc1]

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 64 * 4 * 4)
        x = F.relu(self.fc1(x))
        x = self.output(x)
        return x

    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.head:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals
    
    def get_body_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.body:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals

    def set_head_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.head:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def set_body_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.body:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2


class ResBlock(nn.Module):
    def __init__(self, inchannel, outchannel, stride=1):
        super(ResBlock, self).__init__()
        # è¿™é‡Œå®šä¹‰äº†æ®‹å·®å—å†…è¿ç»­çš„2ä¸ªå·ç§¯å±‚
        self.left = nn.Sequential(
            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(outchannel),
            nn.ReLU(inplace=True),
            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(outchannel)
        )
        self.shortcut = nn.Sequential()
        if stride != 1 or inchannel != outchannel:
            # shortcutï¼Œè¿™é‡Œä¸ºäº†è·Ÿ2ä¸ªå·ç§¯å±‚çš„ç»“æœç»“æ„ä¸€è‡´ï¼Œè¦åšå¤„ç†
            self.shortcut = nn.Sequential(
                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(outchannel)
            )

    def forward(self, x):
        out = self.left(x)
        # å°†2ä¸ªå·ç§¯å±‚çš„è¾“å‡ºè·Ÿå¤„ç†è¿‡çš„xç›¸åŠ ï¼Œå®ç°ResNetçš„åŸºæœ¬ç»“æ„
        out = out + self.shortcut(x)
        out = F.relu(out)

        return out

class Reswithoutcon(nn.Module):
    def __init__(self, option='resnet50', pret=False, with_con=True, num_classes=10):
        super(Reswithoutcon, self).__init__()
        self.dim = 2048
        self.with_con = with_con
        if option == 'resnet18':
            model_ft = models.resnet18(pretrained=pret,num_classes=num_classes,zero_init_residual=True)
            self.dim = 512
        if option == 'resnet34':
            model_ft = models.resnet34(pretrained=pret,num_classes=num_classes,zero_init_residual=True)
            self.dim = 512
        if option == 'resnet50':
            model_ft = models.resnet50(pretrained=pret,num_classes=num_classes,zero_init_residual=True)
        if option == 'resnet101':
            model_ft = models.resnet101(pretrained=pret,num_classes=num_classes,zero_init_residual=True)
        if option == 'resnet152':
            model_ft = models.resnet152(pretrained=pret,num_classes=num_classes,zero_init_residual=True)
        
        mod = list(model_ft.children())
        if with_con:
            temp = mod.pop(0)
            self.features = model_ft
            self.body = temp
            self.head = mod
        else:
            mod = list(model_ft.children())
            mod.pop(0)
            self.class_fit = nn.Sequential(*mod)
            
    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.head:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals
    
    def get_body_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.body:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals

    def set_head_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.head:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def set_body_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.body:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def forward(self, x):
        # x = self.features(x)
        if self.with_con:
            x = self.features(x)
            return x
        else:
            x = self.class_fit(x)
            return x


class MobilenetV2(nn.Module):
    def __init__(self, option='MobilenetV2', pret=False, with_con=True,num_classes=10):
        super(MobilenetV2, self).__init__()
        self.dim = 2048
        self.with_con = with_con
        model_ft = models.mobilenet_v2(pretrained=pret,num_classes=num_classes)
        mod = list(model_ft.children())
        if with_con:
            temp = mod.pop(0)
            self.features = model_ft
            self.body = temp
            self.head = mod
        else:
            mod = list(model_ft.children())
            mod.pop(0)
            self.class_fit = nn.Sequential(*mod)
            
    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.head:
                for temp in bn:
                    if hasattr(temp, 'weight'):
                        vals.append(copy.deepcopy(temp.weight))
                    if hasattr(temp, 'bias'):
                        vals.append(copy.deepcopy(temp.bias))
        return vals
    
    def get_body_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.body:
                for temp in bn:
                    if hasattr(temp, 'weight'):
                        vals.append(copy.deepcopy(temp.weight))
                    if hasattr(temp, 'bias'):
                        vals.append(copy.deepcopy(temp.bias))
        return vals

    def set_head_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.head:
                for temp in bn:
                    if hasattr(temp, 'weight'):
                        temp.weight.copy_(vals[i])
                        i = i + 1
                    if hasattr(temp, 'bias'):
                        temp.bias.copy_(vals[i])
                        i = i + 1

    def set_body_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.body:
                for temp in bn:
                    if hasattr(temp, 'weight'):
                        temp.weight.copy_(vals[i])
                        i = i + 1
                    if hasattr(temp, 'bias'):
                        temp.bias.copy_(vals[i])
                        i = i + 1

    def forward(self, x):
        # x = self.features(x)
        if self.with_con:
            x = self.features(x)
            return x
        else:
            x = self.class_fit(x)
            return x

class ResNet18(nn.Module):
    def __init__(self, num_classes=200):
        super(ResNet18, self).__init__()

        self.loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')

        self.inchannel = 64
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, self.inchannel, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU()
        )
        self.layer1 = self.make_layer(ResBlock, 64, 2, stride=1)
        self.layer2 = self.make_layer(ResBlock, 128, 2, stride=2)
        self.layer3 = self.make_layer(ResBlock, 256, 2, stride=2)
        self.layer4 = self.make_layer(ResBlock, 512, 2, stride=2)
        # self.fc = nn.Linear(512, num_classes).to(device)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

        # self.bn_layers = [self.bn0, self.bn1]
        # self.linear_layers = [self.fc0,self.out]
        # self.deep = [self.bn0, self.bn1,self.out]
        # self.shallow = [self.conv0,self.conv1,self.fc0]
        self.head = [self.fc]
        self.body = [self.layer1, self.layer2, self.layer3, self.layer4]

    # è¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯ç”¨æ¥ï¼Œé‡å¤åŒä¸€ä¸ªæ®‹å·®å—
    def make_layer(self, block, channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.inchannel, channels, stride))
            self.inchannel = channels
        return nn.Sequential(*layers)

    def forward(self, x):
        # åœ¨è¿™é‡Œï¼Œæ•´ä¸ªResNet18çš„ç»“æ„å°±å¾ˆæ¸…æ™°äº†
        out = self.conv1(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        # out = F.avg_pool2d(out, 4)
        # out = out.view(out.size(0), -1)
        out = self.avgpool(out)
        out = torch.flatten(out, 1)
        # print(out.shape)
        out = self.fc(out)
        # print(out)
        return out

    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.head:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals
    
    def get_body_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.body:
                vals.append(copy.deepcopy(bn.weight))
                vals.append(copy.deepcopy(bn.bias))
        return vals

    def set_head_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.head:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def set_body_val(self,vals):
        i = 0
        with torch.no_grad():
            for bn in self.body:
                bn.weight.copy_(vals[i])
                bn.bias.copy_(vals[i+1])
                i = i + 2

    def calc_acc(self, logits, y):
        """
        Calculate top-1 accuracy of model.

        Args:
            - logits: (torch.tensor) unnormalised predictions of y
            - y:      (torch.tensor) true values

        Returns:
            torch.tensor containing scalar value.
        """
        return (torch.argmax(logits, dim=1) == y).float().mean()

    def empty_step(self):
        """
        Perform one step of SGD with all-0 inputs and targets to initialise
        optimiser parameters.
        """
        # self.train_step(torch.zeros((2, 3, 64, 64),
        #                             device=self.device,
        #                             dtype=torch.float32),
        #                 torch.zeros((2),
        #                             device=self.device,
        #                             dtype=torch.int32).long())
        pass


def get_mobilenet(num_classes):
    """
    creates MobileNet model with `n_classes` outputs
    :param num_classes:
    :return: nn.Module
    """
    model = torchvision.models.mobilenet_v2(pretrained=True)
    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)

    return model

class MobileViT(nn.Module):
    """
    MobileViT æ¨¡å‹å°è£…
    
    GPR é€‚é…ï¼š
    - å¯é€‰çš„ GPR é¢„å¤„ç†å±‚ï¼ˆä¿¡å·å½’ä¸€åŒ– + æ—¶ç©ºç‰¹å¾å¢å¼ºï¼‰
    - æ”¯æŒå†»ç»“ backbone åªè®­ç»ƒåˆ†ç±»å¤´
    """
    def __init__(self, model_name='mobilevit_s', num_classes=10, pretrained=True, gpr_mode=False):
        super(MobileViT, self).__init__()
        
        self.gpr_mode = gpr_mode
        
        # GPR ä¸“ç”¨é¢„å¤„ç†å±‚
        if gpr_mode:
            self.gpr_preprocess = nn.Sequential(
                # å¯å­¦ä¹ çš„ä¿¡å·å½’ä¸€åŒ–
                nn.InstanceNorm2d(3, affine=True),
                # æ—¶é—´åŸŸå¢å¼ºï¼ˆå‚ç›´æ–¹å‘ï¼‰
                nn.Conv2d(3, 16, kernel_size=(5, 1), padding=(2, 0), bias=False),
                nn.BatchNorm2d(16),
                nn.ReLU(inplace=True),
                # ç©ºé—´åŸŸå¢å¼ºï¼ˆæ°´å¹³æ–¹å‘ï¼‰
                nn.Conv2d(16, 16, kernel_size=(1, 5), padding=(0, 2), bias=False),
                nn.BatchNorm2d(16),
                nn.ReLU(inplace=True),
                # èåˆå› 3 é€šé“
                nn.Conv2d(16, 3, kernel_size=1, bias=False),
                nn.BatchNorm2d(3),
            )
        
        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)
        
        # FedDWA éœ€è¦åˆ†ç¦» head å’Œ body
        # å¯¹äº timm çš„ mobilevitï¼Œé€šå¸¸ classifier æ˜¯ head
        # æˆ‘ä»¬éœ€è¦æ£€æŸ¥å…·ä½“ç»“æ„ï¼Œè¿™é‡Œå‡è®¾æ˜¯æ ‡å‡†çš„ timm ç»“æ„
        
        # å°è¯•è‡ªåŠ¨è¯†åˆ« head å’Œ body
        if hasattr(self.model, 'head'):
            self.head = [self.model.head]
            # body æ˜¯é™¤äº† head ä¹‹å¤–çš„æ‰€æœ‰éƒ¨åˆ†ï¼Œè¿™æ¯”è¾ƒéš¾ç›´æ¥è·å–åˆ—è¡¨
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬æŠŠæ•´ä¸ª model å½“ä½œ featuresï¼Œé™¤äº† head
            # ä½† FedDWA éœ€è¦å‚æ•°åˆ—è¡¨ã€‚
            # è®©æˆ‘ä»¬ç”¨ named_children æ¥åŒºåˆ†
            self.body = [m for n, m in self.model.named_children() if n != 'head']
        elif hasattr(self.model, 'classifier'): # MobileNetV3 ç­‰
             self.head = [self.model.classifier]
             self.body = [m for n, m in self.model.named_children() if n != 'classifier']
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æ˜æ˜¾çš„ headï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æŒ‡å®šï¼Œæˆ–è€…æŠŠæœ€åçš„å…¨è¿æ¥å±‚å½“ä½œ head
            # è¿™é‡Œåšä¸€ä¸ªé€šç”¨çš„ fallbackï¼Œå‡è®¾æœ€åä¸€å±‚æ˜¯ head
            children = list(self.model.children())
            self.head = [children[-1]]
            self.body = children[:-1]

    def forward(self, x):
        if self.gpr_mode:
            x = self.gpr_preprocess(x)
        return self.model(x)

    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.head:
                for param in bn.parameters():
                    vals.append(copy.deepcopy(param))
        return vals
    
    def get_body_val(self):
        vals = []
        with torch.no_grad():
            for bn in self.body:
                for param in bn.parameters():
                    vals.append(copy.deepcopy(param))
        return vals

    def set_head_val(self, vals):
        i = 0
        with torch.no_grad():
            for bn in self.head:
                for param in bn.parameters():
                    param.copy_(vals[i])
                    i += 1

    def set_body_val(self, vals):
        i = 0
        with torch.no_grad():
            for bn in self.body:
                for param in bn.parameters():
                    param.copy_(vals[i])
                    i += 1

try:
    import clip
except ImportError:
    clip = None

import math

# [æ–°å¢] äºŒå€¼åŒ–æ­¥é•¿å‡½æ•° (è¿™æ˜¯ MaskedMLP å®ç°ç¨€ç–æ€§çš„æ ¸å¿ƒ)
class BinaryStep(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        # åªæœ‰æƒé‡å¤§äº 0.01 çš„è¿æ¥æ‰ä¼šè¢«ä¿ç•™ï¼Œå…¶ä»–çš„ä¼šè¢«â€œå‰ªæ–­â€
        return (input > 0.01).float()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        return grad_input

# [ä¿®æ”¹] å®Œæ•´ç‰ˆ MaskedMLP (å¤åˆ» FedMedCLIP)
class MaskedMLP(nn.Module):
    def __init__(self, in_size, out_size):
        super(MaskedMLP, self).__init__()
        self.in_size = in_size
        self.out_size = out_size
        self.weight = nn.Parameter(torch.Tensor(out_size, in_size))
        self.bias = nn.Parameter(torch.Tensor(out_size))
        # å¯å­¦ä¹ çš„é˜ˆå€¼ï¼Œæ§åˆ¶å‰ªæçš„åŠ›åº¦
        self.threshold = nn.Parameter(torch.Tensor(out_size)) 
        self.step = BinaryStep.apply 
        self.mask = torch.ones(out_size, in_size)
        
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)
        with torch.no_grad():
            self.threshold.data.fill_(0.)

    def mask_generation(self):
        # åŠ¨æ€ç”Ÿæˆæ©ç ï¼šåªæœ‰æƒé‡ç»å¯¹å€¼å¤§äºé˜ˆå€¼çš„è¿æ¥æ‰ç”Ÿæ•ˆ
        abs_weight = torch.abs(self.weight)
        threshold = self.threshold.view(abs_weight.shape[0], -1)
        abs_weight = abs_weight - threshold
        mask = self.step(abs_weight)
        self.mask = mask.to(self.weight.device)

    def forward(self, input):
        # æ¯æ¬¡å‰å‘ä¼ æ’­å‰ï¼Œå…ˆç”Ÿæˆæ©ç 
        self.mask_generation() 
        masked_weight = self.weight * self.mask
        return F.linear(input, masked_weight, self.bias)


class TextEncoder(nn.Module):
    def __init__(self, clip_model):
        super().__init__()
        self.transformer = clip_model.transformer
        self.positional_embedding = clip_model.positional_embedding
        self.ln_final = clip_model.ln_final
        self.text_projection = clip_model.text_projection
        self.dtype = clip_model.dtype

    def forward(self, prompts, tokenized_prompts):
        x = prompts + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        indices = tokenized_prompts.argmax(dim=-1)
        batch_indices = torch.arange(x.shape[0], device=x.device)
        
        x = x[batch_indices, indices] @ self.text_projection

        return x

# class PromptLearner(nn.Module):
#     def __init__(self, classnames, clip_model, n_ctx=16, csc=False, class_token_position='end'):
#         super().__init__()
#         n_cls = len(classnames)
#         dtype = clip_model.dtype
#         ctx_dim = clip_model.ln_final.weight.shape[0]
        
#         if csc:
#             # print("Initializing class-specific contexts")
#             ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim, dtype=dtype)
#         else:
#             # print("Initializing a generic context")
#             ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)
#         nn.init.normal_(ctx_vectors, std=0.02)
        
#         prompt_prefix = " ".join(["X"] * n_ctx)
#         self.ctx = nn.Parameter(ctx_vectors)

#         classnames = [name.replace("_", " ") for name in classnames]
#         prompts = [prompt_prefix + " " + name + "." for name in classnames]

#         tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])
#         with torch.no_grad():
#             device = next(clip_model.parameters()).device
#             embedding = clip_model.token_embedding(tokenized_prompts.to(device)).type(dtype)

#         self.register_buffer("token_prefix", embedding[:, :1, :])
#         self.register_buffer("token_suffix", embedding[:, 1 + n_ctx:, :])
#         self.register_buffer("tokenized_prompts", tokenized_prompts)

#         self.n_cls = n_cls
#         self.n_ctx = n_ctx
#         self.class_token_position = class_token_position
#         self.csc = csc

#     def forward(self):
#         ctx = self.ctx
#         if ctx.dim() == 2:
#             if self.csc:
#                 ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)
#             else:
#                  ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)
        
#         prefix = self.token_prefix
#         suffix = self.token_suffix

#         if self.class_token_position == "end":
#             prompts = torch.cat([prefix, ctx, suffix], dim=1)
#         elif self.class_token_position == "middle":
#             half_n_ctx = self.n_ctx // 2
#             prompts = torch.cat([prefix, ctx[:, :half_n_ctx], suffix[:, : -1 - half_n_ctx], ctx[:, half_n_ctx:], suffix[:, -1:]], dim=1)
#         elif self.class_token_position == "front":
#             prompts = torch.cat([prefix, suffix[:, : -1 - self.n_ctx], ctx, suffix[:, -1:]], dim=1)
#         else:
#             raise ValueError

#         return prompts
class PromptLearner(nn.Module):
    def __init__(self, classnames, clip_model, n_ctx=16, csc=False, class_token_position='end'):
        super().__init__()
        n_cls = len(classnames)
        dtype = clip_model.dtype
        ctx_dim = clip_model.ln_final.weight.shape[0]
        
        # ==================== [ğŸš€ GPR-CoOp æ ¸å¿ƒä¿®æ”¹] ====================
        # å®šä¹‰ GPR é¢†åŸŸçš„ç‰©ç†å­¦â€œè¡Œè¯â€ä½œä¸ºåˆå§‹åŒ–é”šç‚¹
        # è¿™å¥è¯åŒ…å«äº† GPR å›¾åƒçš„æ ¸å¿ƒç‰¹å¾ï¼šB-scan, signal, subsurface, reflection
        # é•¿åº¦åˆšå¥½çº¦ 10-12 ä¸ª tokenï¼Œé€‚åˆ n_ctx=16 çš„è®¾ç½®
        gpr_init_text = "GPR B-scan signal showing subsurface dielectric reflection"
        
        print(f"[GPR-CoOp] Initializing Context with Physics Prior: '{gpr_init_text}'")
        
        # 1. å°†ç‰©ç†æè¿°ç¼–ç ä¸º Embedding
        with torch.no_grad():
            # è·å– deviceï¼Œé˜²æ­¢è·¨è®¾å¤‡é”™è¯¯
            device = next(clip_model.parameters()).device
            tokenized_init = clip.tokenize(gpr_init_text).to(device)
            embedding = clip_model.token_embedding(tokenized_init).type(dtype)
        
        # 2. æˆªå–æœ‰æ•ˆå‘é‡ä½œä¸ºåˆå§‹å€¼ (å»æ‰ SOS [Start] token)
        # embedding shape: [1, 77, 512]
        # æˆ‘ä»¬å–å‰ n_ctx ä¸ª token çš„å‘é‡ã€‚å¦‚æœ init_text ä¸å¤Ÿé•¿ï¼ŒCLIP ä¼šç”¨ padding å¡«å……ï¼Œä¹Ÿæ²¡å…³ç³»ã€‚
        # å¦‚æœ init_text æ¯” n_ctx é•¿ï¼Œè¿™å°±æˆªæ–­äº†ã€‚
        n_init = min(n_ctx, embedding.shape[1] - 2) # ä¿é™©èµ·è§å‡å» SOS/EOS
        
        # åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„ ctx_vectors
        if csc:
            ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim, dtype=dtype)
        else:
            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)
            
        # 3. å¡«å……ç‰©ç†åˆå§‹åŒ–å‘é‡
        # å…ˆç”¨æ­£æ€åˆ†å¸ƒæ‰“åº•ï¼ˆé˜²æ­¢å…¨é›¶æ¢¯åº¦é—®é¢˜ï¼‰
        nn.init.normal_(ctx_vectors, std=0.02)
        
        # ç„¶åæŠŠç‰©ç†å‘é‡å¡«è¿›å»è¦†ç›–æ‰å‰ n_init ä¸ªä½ç½®
        physics_vectors = embedding[0, 1:1+n_init, :] # [n_init, dim]
        
        if csc:
            # ç±»åˆ«ç‰¹æœ‰æ¨¡å¼ï¼šæ¯ä¸ªç±»éƒ½ä»è¿™ä¸ªç‰©ç†èµ·ç‚¹å¼€å§‹
            for i in range(n_cls):
                ctx_vectors[i, :n_init, :] = physics_vectors
        else:
            # ç»Ÿä¸€æ¨¡å¼
            ctx_vectors[:n_init, :] = physics_vectors
            
        print(f"[GPR-CoOp] Physics initialization applied to first {n_init} tokens.")
        
        # ==================== [ä¿®æ”¹ç»“æŸ] ====================

        self.ctx = nn.Parameter(ctx_vectors) # æ³¨å†Œä¸ºå¯è®­ç»ƒå‚æ•°
        # ==================== [ğŸš¨ ä¿®å¤è¡¥ä¸ï¼šåŠ å›è¿™è¡Œä»£ç ] ====================
        # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªé•¿åº¦ä¸º n_ctx çš„å ä½ç¬¦å­—ç¬¦ä¸²ï¼Œç”¨æ¥ç”Ÿæˆ Token åºåˆ—
        # è™½ç„¶å®ƒçš„ Embedding ä¼šè¢«æˆ‘ä»¬ä¸Šé¢çš„ self.ctx æ›¿ä»£ï¼Œä½† Tokenizer éœ€è¦å®ƒæ¥ç¡®å®šé•¿åº¦å’Œä½ç½®
        prompt_prefix = " ".join(["X"] * n_ctx) 
        # ===================================================================

        classnames = [name.replace("_", " ") for name in classnames]
        prompts = [prompt_prefix + " " + name + "." for name in classnames]

        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])
        with torch.no_grad():
            device = next(clip_model.parameters()).device
            embedding = clip_model.token_embedding(tokenized_prompts.to(device)).type(dtype)

        self.register_buffer("token_prefix", embedding[:, :1, :])  # SOS
        self.register_buffer("token_suffix", embedding[:, 1 + n_ctx:, :])  # CLS, EOS
        self.register_buffer("tokenized_prompts", tokenized_prompts)

        self.n_cls = n_cls
        self.n_ctx = n_ctx
        self.class_token_position = class_token_position
        self.csc = csc

    def forward(self):
        ctx = self.ctx
        if ctx.dim() == 2:
            if self.csc:
                ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)
            else:
                 ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)
        
        prefix = self.token_prefix
        suffix = self.token_suffix

        if self.class_token_position == "end":
            prompts = torch.cat(
                [
                    prefix,  # (n_cls, 1, dim)
                    ctx,     # (n_cls, n_ctx, dim)
                    suffix,  # (n_cls, *, dim)
                ],
                dim=1,
            )
        elif self.class_token_position == "middle":
            half_n_ctx = self.n_ctx // 2
            prompts = torch.cat(
                [
                    prefix,
                    ctx[:, :half_n_ctx],
                    suffix[:, : -1 - half_n_ctx],
                    ctx[:, half_n_ctx:],
                    suffix[:, -1:],
                ],
                dim=1,
            )
        elif self.class_token_position == "front":
            prompts = torch.cat(
                [
                    prefix,
                    suffix[:, : -1 - self.n_ctx],
                    ctx,
                    suffix[:, -1:],
                ],
                dim=1,
            )
        else:
            raise ValueError

        return prompts


class FedCLIP(nn.Module):
    """
    FedCLIP (SOTAç‰ˆ): 
    1. ä½¿ç”¨ Semantic-Consistent Attention Adapter (SCAA) ä¿æŠ¤é¢„è®­ç»ƒçŸ¥è¯†
    2. ä½¿ç”¨ Prompt Ensemble å¢å¼ºæ–‡æœ¬é²æ£’æ€§
    """
    def __init__(self, model_name='ViT-B/32', device='cuda', num_classes=10, class_names=None, gpr_mode=False, use_coop=False, n_ctx=16, csc=False, class_token_position='end'):
        super(FedCLIP, self).__init__()
        if clip is None:
            raise ImportError("Please install clip: pip install git+https://github.com/openai/CLIP.git")
            
        self.device = device
        self.gpr_mode = gpr_mode
        self.use_coop = use_coop
        self.n_ctx = n_ctx
        self.csc = csc
        self.class_token_position = class_token_position
        
        # Load CLIP model
        self.model, self.preprocess = clip.load(model_name, device=device, jit=False)
        self.model.eval() 
        for param in self.model.parameters():
            param.requires_grad = False
            
        # Initialize PromptLearner
        if use_coop and class_names:
             self.prompt_learner = PromptLearner(class_names, self.model, n_ctx=n_ctx, csc=csc, class_token_position=class_token_position)
             self.text_encoder = TextEncoder(self.model)
        else:
             self.prompt_learner = None
             self.text_encoder = None
            
        # Infer dim
        if model_name == 'ViT-B/32':
            dim = 512
        elif model_name == 'ViT-L/14':
            dim = 768
        else:
            with torch.no_grad():
                dummy = torch.zeros(1, 3, 224, 224).to(device)
                dim = self.model.encode_image(dummy).shape[1]

        self.dim = dim
        
        # ============================================================
        # [ğŸ› ï¸ æ ¸å¿ƒä¿®æ”¹ 1: ç»Ÿä¸€ Adapter æ¶æ„]
        # å¼ºåˆ¶ä½¿ç”¨å®Œæ•´ç‰ˆ MaskedMLP + Softmax æ³¨æ„åŠ›ç»“æ„
        # åˆ é™¤äº† GPRAdapter åˆ†æ”¯
        # ============================================================
        self.fea_attn = nn.Sequential(
            MaskedMLP(dim, dim),
            nn.BatchNorm1d(dim),
            nn.ReLU(),
            MaskedMLP(dim, dim),
            nn.Softmax(dim=1)
        )
        
        self.class_names = class_names
        self.text_features = None
        self.num_classes = num_classes
        
        if self.class_names:
            self.set_class_prompts(self.class_names)
   
             
    # def set_class_prompts(self, class_names):
    #     self.class_names = class_names
        
    #     if self.use_coop:
    #         if self.prompt_learner is None:
    #              self.prompt_learner = PromptLearner(class_names, self.model, n_ctx=self.n_ctx, csc=self.csc, class_token_position=self.class_token_position)
    #              self.text_encoder = TextEncoder(self.model)
    #              self.prompt_learner.to(self.device)
    #              self.text_encoder.to(self.device)
    #         return

    #     # ============================================================
    #     # [ğŸ› ï¸ æ ¸å¿ƒä¿®æ”¹ 2: Prompt Ensemble]
    #     # åˆ é™¤äº† custom_gpr_prompts å­—å…¸ï¼Œåªä½¿ç”¨æ¨¡æ¿é›†æˆ
    #     # ============================================================
    #     templates = [
    #         "a picture of a {}.",                 
    #         "a ground penetrating radar image of {}.", 
    #         "a GPR scan of {}.",                  
    #         "underground image of {}.",           
    #         "geological data showing {}."         
    #     ]

    #     all_text_features = []
    #     with torch.no_grad():
    #         for c in class_names:
    #             prompts = [template.format(c) for template in templates]
    #             text_tokens = clip.tokenize(prompts).to(self.device)
    #             class_embeddings = self.model.encode_text(text_tokens)
    #             class_embeddings = class_embeddings / class_embeddings.norm(dim=1, keepdim=True)
                
    #             # Mean Pooling
    #             mean_embedding = class_embeddings.mean(dim=0)
    #             mean_embedding = mean_embedding / mean_embedding.norm()
    #             all_text_features.append(mean_embedding)
            
    #         self.text_features = torch.stack(all_text_features).float()

    def set_class_prompts(self, class_names):
        self.class_names = class_names
        
        if self.use_coop:
            if self.prompt_learner is None:
                 self.prompt_learner = PromptLearner(class_names, self.model, n_ctx=self.n_ctx, csc=self.csc, class_token_position=self.class_token_position)
                 self.text_encoder = TextEncoder(self.model)
                 self.prompt_learner.to(self.device)
                 self.text_encoder.to(self.device)
            return
        
        # [âœ… æ¢å¤] æ—¢ç„¶å®éªŒè¯æ˜ç‰©ç†æè¿°æœ‰æ•ˆ (92%)ï¼Œæˆ‘ä»¬ä¿ç•™å®ƒä½œä¸ºé¢†åŸŸçŸ¥è¯†å¢å¼º
        custom_gpr_prompts = {
            "Loose": ["GPR signal of loose uncompacted soil", "low density area in ground penetrating radar", "scattered reflections indicating loose material"],
            "Crack": ["GPR B-scan showing a hyperbolic reflection from a crack", "discontinuity in subsurface layers indicating a fracture", "vertical crack signature in radargram"],
            "Mud Pumping": ["GPR signature of mud pumping under pavement", "subsurface moisture and fine material accumulation", "blurred reflection caused by mud pumping"],
            "Pipeline": ["hyperbolic reflection from a buried pipeline", "GPR scan of an underground pipe", "inverted U-shape reflection of a utility line"],
            "Redar": ["a specific radar anomaly", "ground penetrating radar target", "distinctive GPR reflection pattern"],
            "stell_rib": ["strong hyperbolic reflection from a steel rib", "GPR image of metal reinforcement bar", "regularly spaced high amplitude reflections from steel"],
            "Void": ["GPR image showing a subsurface void", "signal ringing and polarity reversal indicating a cavity", "empty space underground in radargram"],
            "Water Abnormality": ["GPR signal attenuation caused by water saturation", "high dielectric contrast area indicating water abnormality", "subsurface water leakage signature"]
        }

        # [ä¿ç•™] é€šç”¨æ¨¡æ¿ä½œä¸ºè¡¥å……
        templates = [
            "a ground penetrating radar image showing {}",
            "a GPR B-scan of {}",
            "a radargram containing {}",
            "subsurface detection of {}",
            "a GPR profile with {}",
            "geophysical data showing {}",
        ]
            
        all_text_features = []
        
        with torch.no_grad():
            for c in class_names:
                # 1. ä¼˜å…ˆè·å–è‡ªå®šä¹‰æè¿°
                prompt_list = custom_gpr_prompts.get(c, [])
                
                # 2. å¦‚æœæ²¡æœ‰è‡ªå®šä¹‰æè¿°ï¼Œæˆ–è€…æƒ³æ··åˆä½¿ç”¨ï¼Œè¿™é‡ŒæŠŠæ¨¡æ¿ç”Ÿæˆçš„ä¹ŸåŠ è¿›å»
                # ç­–ç•¥ï¼šæ··åˆä¸“å®¶çŸ¥è¯†ä¸é€šç”¨æ¨¡æ¿ (Expert + General Ensemble)
                template_prompts = [t.format(c) for t in templates]
                
                # åˆå¹¶æ‰€æœ‰ Prompt
                final_prompts = prompt_list + template_prompts
                
                # ç¼–ç 
                text_tokens = clip.tokenize(final_prompts).to(self.device)
                class_embeddings = self.model.encode_text(text_tokens)
                class_embeddings = class_embeddings / class_embeddings.norm(dim=1, keepdim=True)
                
                # å–å¹³å‡
                mean_embedding = class_embeddings.mean(dim=0)
                mean_embedding = mean_embedding / mean_embedding.norm()
                
                all_text_features.append(mean_embedding)
            
            self.text_features = torch.stack(all_text_features).float()
            
    def forward(self, x, return_features=False):
        with torch.no_grad():
            original_image_features = self.model.encode_image(x).float()
            
        # ============================================================
        # [ğŸ› ï¸ æ ¸å¿ƒä¿®æ”¹ 3: æ³¨æ„åŠ›ä¹˜æ³•é€»è¾‘]
        # ============================================================
        attn_weights = self.fea_attn(original_image_features)
        image_features = torch.mul(attn_weights, original_image_features)

        # 3. å½’ä¸€åŒ–
        image_features = image_features / image_features.norm(dim=1, keepdim=True)

        # 4. è·å–æ–‡æœ¬ç‰¹å¾
        if self.use_coop and self.prompt_learner is not None:
            prompts = self.prompt_learner()
            tokenized_prompts = self.prompt_learner.tokenized_prompts
            text_features = self.text_encoder(prompts, tokenized_prompts)
        else:
            text_features = self.text_features

        if text_features is None:
             if self.training: raise ValueError("Prompts not set.")
             return torch.zeros(x.size(0), self.num_classes).to(self.device)
        
        text_features = text_features.float()
        text_features = text_features / text_features.norm(dim=1, keepdim=True)

        # 5. è®¡ç®— Logits
        logit_scale = self.model.logit_scale.exp().float()
        logits = logit_scale * image_features @ text_features.t()
        
        if return_features:
            return logits, image_features
        
        return logits
        
    # FedDWA interfaces
    def get_head_val(self):
        vals = []
        with torch.no_grad():
            for param in self.fea_attn.parameters():
                vals.append(copy.deepcopy(param))
            if self.use_coop and self.prompt_learner is not None:
                for param in self.prompt_learner.parameters():
                    vals.append(copy.deepcopy(param))     
        return vals
        
    def set_head_val(self, vals):
        i = 0
        with torch.no_grad():
            for param in self.fea_attn.parameters():
                param.copy_(vals[i])
                i += 1
            if self.use_coop and self.prompt_learner is not None:
                for param in self.prompt_learner.parameters():
                    param.copy_(vals[i])
                    i += 1
                
    def get_body_val(self):
        return []

    def set_body_val(self, vals):
        pass
# ============================================================================
# GPR-FedSense: ä¸“ä¸ºæ¢åœ°é›·è¾¾æ•°æ®è®¾è®¡çš„è”é‚¦å­¦ä¹ æ¶æ„
# ============================================================================

class GPRSignalNorm(nn.Module):
    """
    GPR ä¿¡å·å½’ä¸€åŒ–å±‚
    å¯å­¦ä¹ çš„å½’ä¸€åŒ–å‚æ•°ï¼Œé€‚é…ä¸åŒè®¾å¤‡/ç¯å¢ƒçš„ä¿¡å·ç‰¹æ€§
    """
    def __init__(self, num_features):
        super(GPRSignalNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(1, num_features, 1, 1))
        self.beta = nn.Parameter(torch.zeros(1, num_features, 1, 1))
        # å¯å­¦ä¹ çš„ä¿¡å·å¢ç›Šæ ¡æ­£
        self.gain = nn.Parameter(torch.ones(1))
        
    def forward(self, x):
        # å®ä¾‹å½’ä¸€åŒ– (é€‚é…å•æ ·æœ¬çš„è®¾å¤‡å·®å¼‚)
        mean = x.mean(dim=[2, 3], keepdim=True)
        std = x.std(dim=[2, 3], keepdim=True) + 1e-5
        x = (x - mean) / std
        x = x * self.gamma + self.beta
        x = x * self.gain
        return x


class GPRFeatureExtractor(nn.Module):
    """
    GPR ä¸“ç”¨ç‰¹å¾æå–å™¨
    ç»“åˆ 1Dï¼ˆæ—¶é—´åŸŸï¼‰å’Œ 2Dï¼ˆç©ºé—´åŸŸï¼‰å·ç§¯ï¼Œæ•è· GPR ä¿¡å·çš„æ—¶é¢‘ç‰¹å¾
    
    è®¾è®¡ç†å¿µï¼š
    - æµ…å±‚ï¼š1D å·ç§¯æå–æ—¶é—´åŸŸåå°„ç‰¹å¾
    - ä¸­å±‚ï¼š2D å·ç§¯æå–ç©ºé—´ç»“æ„ç‰¹å¾
    - æ·±å±‚ï¼šæ··åˆæ³¨æ„åŠ›å¢å¼ºå…³é”®åŒºåŸŸ
    """
    def __init__(self, in_channels=3, base_dim=64):
        super(GPRFeatureExtractor, self).__init__()
        
        # å¯å­¦ä¹ çš„ä¿¡å·å½’ä¸€åŒ–ï¼ˆé€‚é…ä¸åŒè®¾å¤‡ï¼‰
        self.signal_norm = GPRSignalNorm(in_channels)
        
        # Stage 1: æµ…å±‚ç‰¹å¾ï¼ˆæ•è·è¾¹ç¼˜å’Œçº¹ç†ï¼‰
        self.stage1 = nn.Sequential(
            nn.Conv2d(in_channels, base_dim, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(base_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(base_dim, base_dim, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(base_dim),
            nn.ReLU(inplace=True),
        )
        
        # Stage 2: æ—¶é—´åŸŸç‰¹å¾ï¼ˆå‚ç›´æ–¹å‘å·ç§¯ï¼Œæ•è·æ·±åº¦åå°„ï¼‰
        self.time_conv = nn.Sequential(
            nn.Conv2d(base_dim, base_dim, kernel_size=(5, 1), padding=(2, 0), bias=False),
            nn.BatchNorm2d(base_dim),
            nn.ReLU(inplace=True),
        )
        
        # Stage 3: ç©ºé—´åŸŸç‰¹å¾ï¼ˆæ°´å¹³æ–¹å‘å·ç§¯ï¼Œæ•è·æ¨ªå‘å»¶ç»­æ€§ï¼‰
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(base_dim, base_dim, kernel_size=(1, 5), padding=(0, 2), bias=False),
            nn.BatchNorm2d(base_dim),
            nn.ReLU(inplace=True),
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Conv2d(base_dim * 2, base_dim * 2, kernel_size=1, bias=False),
            nn.BatchNorm2d(base_dim * 2),
            nn.ReLU(inplace=True),
        )
        
        # è¾“å‡ºç»´åº¦
        self.out_channels = base_dim * 2
        
    def forward(self, x):
        # ä¿¡å·å½’ä¸€åŒ–
        x = self.signal_norm(x)
        
        # Stage 1
        x = self.stage1(x)
        
        # å¹¶è¡Œçš„æ—¶é—´/ç©ºé—´ç‰¹å¾æå–
        time_feat = self.time_conv(x)
        spatial_feat = self.spatial_conv(x)
        
        # ç‰¹å¾èåˆ
        x = torch.cat([time_feat, spatial_feat], dim=1)
        x = self.fusion(x)
        
        return x


class GPRFedModel(nn.Module):
    """
    GPR-FedSense: æ¢åœ°é›·è¾¾è”é‚¦å­¦ä¹ ä¸“ç”¨æ¨¡å‹
    
    æ¶æ„ç‰¹ç‚¹ï¼š
    1. æœ¬åœ°ç§æœ‰å±‚ï¼šGPR ä¿¡å·å½’ä¸€åŒ– + ç‰¹å¾æå–ï¼ˆé€‚é…ä¸åŒè®¾å¤‡/ç¯å¢ƒï¼‰
    2. å…¨å±€å…±äº«å±‚ï¼šæ·±å±‚ç‰¹å¾æå–ï¼ˆè·¨å®¢æˆ·ç«¯çŸ¥è¯†å…±äº«ï¼‰
    3. ä¸ªæ€§åŒ–åˆ†ç±»å¤´ï¼šALA è‡ªé€‚åº”èšåˆï¼ˆå¤„ç† Non-IIDï¼‰
    
    Args:
        num_classes: åˆ†ç±»ç±»åˆ«æ•°
        base_dim: åŸºç¡€é€šé“æ•°
        backbone: å…±äº«å±‚ backbone ç±»å‹ ('cnn', 'resnet18', 'mobilevit')
        pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    """
    def __init__(self, num_classes=8, base_dim=64, backbone='cnn', pretrained=True, image_size=224):
        super(GPRFedModel, self).__init__()
        
        self.num_classes = num_classes
        self.backbone_type = backbone
        
        # ============ æ¨¡å— 1: GPR æœ¬åœ°ç‰¹å¾æå–å™¨ (ç§æœ‰ï¼Œä¸èšåˆ) ============
        self.local_extractor = GPRFeatureExtractor(in_channels=3, base_dim=base_dim)
        local_out_dim = self.local_extractor.out_channels  # 128
        
        # ============ æ¨¡å— 2: å…±äº« Backbone (å…¨å±€èšåˆ) ============
        if backbone == 'cnn':
            self.shared_backbone = nn.Sequential(
                nn.Conv2d(local_out_dim, 256, kernel_size=3, stride=2, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True),
                nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),
                nn.BatchNorm2d(512),
                nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((1, 1)),
            )
            feature_dim = 512
            
        elif backbone == 'resnet18':
            # ä½¿ç”¨ ResNet18ï¼Œä½†æ›¿æ¢ç¬¬ä¸€å±‚ä»¥æ¥æ”¶ local_extractor çš„è¾“å‡º
            resnet = models.resnet18(pretrained=pretrained)
            resnet.conv1 = nn.Conv2d(local_out_dim, 64, kernel_size=7, stride=2, padding=3, bias=False)
            # ç§»é™¤åŸå§‹çš„ fc å±‚
            self.shared_backbone = nn.Sequential(*list(resnet.children())[:-1])
            feature_dim = 512
            
        elif backbone == 'mobilevit':
            # ä½¿ç”¨ MobileViTï¼Œä½†éœ€è¦é€‚é…è¾“å…¥é€šé“
            self.adapter_conv = nn.Conv2d(local_out_dim, 3, kernel_size=1)  # è½¬æ¢å› 3 é€šé“
            self.shared_backbone = timm.create_model('mobilevitv2_050', pretrained=pretrained, num_classes=0)
            feature_dim = self.shared_backbone.num_features
            
        else:
            raise ValueError(f"Unknown backbone: {backbone}")
            
        self.feature_dim = feature_dim
        
        # ============ æ¨¡å— 3: ä¸ªæ€§åŒ–åˆ†ç±»å¤´ (æœ¬åœ°å¾®è°ƒ + ALA) ============
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(feature_dim, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(256, num_classes),
        )
        
        # ç”¨äº FedDecorr çš„ç‰¹å¾è¾“å‡ºé’©å­
        self.features = None
        
    def forward(self, x, return_features=False):
        # æœ¬åœ°ç‰¹å¾æå–
        x = self.local_extractor(x)
        
        # å…±äº« backbone
        if self.backbone_type == 'mobilevit':
            x = self.adapter_conv(x)
        x = self.shared_backbone(x)
        
        # å±•å¹³
        if x.dim() > 2:
            x = x.view(x.size(0), -1)
            
        # ä¿å­˜ç‰¹å¾ç”¨äº FedDecorr
        self.features = x
        
        # åˆ†ç±»
        out = self.classifier(x)
        
        if return_features:
            return out, x
        return out
    
    def get_features(self):
        """è·å–æœ€åä¸€å±‚ç‰¹å¾ï¼Œç”¨äº FedDecorr"""
        return self.features
    
    # ============ FedDWA æ¥å£ ============
    def get_head_val(self):
        """è·å–åˆ†ç±»å¤´å‚æ•°ï¼ˆç”¨äºä¸ªæ€§åŒ–èšåˆï¼‰"""
        vals = []
        with torch.no_grad():
            for param in self.classifier.parameters():
                vals.append(copy.deepcopy(param))
        return vals
    
    def set_head_val(self, vals):
        """è®¾ç½®åˆ†ç±»å¤´å‚æ•°"""
        i = 0
        with torch.no_grad():
            for param in self.classifier.parameters():
                param.copy_(vals[i])
                i += 1
                
    def get_body_val(self):
        """è·å–å…±äº«å±‚å‚æ•°ï¼ˆç”¨äºå…¨å±€èšåˆï¼‰"""
        vals = []
        with torch.no_grad():
            for param in self.shared_backbone.parameters():
                vals.append(copy.deepcopy(param))
        return vals
    
    def set_body_val(self, vals):
        """è®¾ç½®å…±äº«å±‚å‚æ•°"""
        i = 0
        with torch.no_grad():
            for param in self.shared_backbone.parameters():
                param.copy_(vals[i])
                i += 1
                
    def get_local_val(self):
        """è·å–æœ¬åœ°ç§æœ‰å±‚å‚æ•°ï¼ˆä¸å‚ä¸èšåˆï¼‰"""
        vals = []
        with torch.no_grad():
            for param in self.local_extractor.parameters():
                vals.append(copy.deepcopy(param))
        return vals
    
    def set_local_val(self, vals):
        """è®¾ç½®æœ¬åœ°ç§æœ‰å±‚å‚æ•°"""
        i = 0
        with torch.no_grad():
            for param in self.local_extractor.parameters():
                param.copy_(vals[i])
                i += 1
