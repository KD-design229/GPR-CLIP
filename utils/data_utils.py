import numpy as np
import torch
import random
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Subset

# [ ğŸ›¡ï¸ åŸºç¡€è®¾æ–½: Subset å…¼å®¹æ€§è¡¥ä¸ ]
def get_targets_safe(dataset):
    if hasattr(dataset, 'targets'):
        return np.array(dataset.targets)
    if isinstance(dataset, Subset):
        if hasattr(dataset.dataset, 'targets'):
            return np.array(dataset.dataset.targets)[dataset.indices]
    loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=2)
    targets = []
    for _, y in loader:
        targets.extend(y.numpy())
    return np.array(targets)

def noniid_type8(datasetname, dataset, num_users, num_classes=10, sample_assignment=None, test=False, logger=None):
    dataset_image = []
    dataset_label = []
    dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=2)

    for _, data in enumerate(dataloader, 0):
        dataset_data, dataset_targets = data
    
    dataset_image.extend(np.array(dataset_data))
    dataset_label.extend(np.array(dataset_targets))
    dataset_image = np.array(dataset_image)
    dataset_label = np.array(dataset_label)

    num_shards = int(num_users * 2)
    order = np.argsort(dataset_label)
    x_sorted = dataset_image[order]
    y_sorted = dataset_label[order]

    n_shards = num_users * 2
    x_shards = np.array_split(x_sorted, n_shards)
    y_shards = np.array_split(y_sorted, n_shards)
    
    if sample_assignment is None:
        sample_assignment = np.array_split(np.random.permutation(n_shards), num_users)

    data = []
    for w in range(num_users):
        indices = sample_assignment[w]
        X = np.concatenate([x_shards[i] for i in indices])
        y = np.concatenate([y_shards[i] for i in indices])
        if logger: logger.info(np.unique(y))
        X = torch.tensor(X, dtype=torch.float32)
        y = torch.tensor(y, dtype=torch.int64)
        data.append([(x, y) for x, y in zip(X, y)])
    return data, sample_assignment

def noniid_type9(datasetname, trainset, testset, num_users, num_classes=10, dirichlet_alpha=0.1, least_samples=20, logger=None):
    train_labels = get_targets_safe(trainset)
    test_labels = get_targets_safe(testset)
    
    trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False, num_workers=2)
    testloader = DataLoader(testset, batch_size=len(testset), shuffle=False, num_workers=2)

    dataset_image = []
    for _, (data, _) in enumerate(trainloader): dataset_image.extend(np.array(data))
    for _, (data, _) in enumerate(testloader): dataset_image.extend(np.array(data))
    dataset_image = np.array(dataset_image)
    dataset_label = np.concatenate((train_labels, test_labels))

    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}
    min_size = 0
    K = num_classes
    N = len(dataset_label)

    while min_size < least_samples:
        idx_batch = [[] for _ in range(num_users)]
        for k in range(K):
            idx_k = np.where(dataset_label == k)[0]
            np.random.shuffle(idx_k)
            proportions = np.random.dirichlet(np.repeat(dirichlet_alpha, num_users))
            proportions = np.array([p * (len(idx_j) < N / num_users) for p, idx_j in zip(proportions, idx_batch)])
            proportions = proportions / proportions.sum()
            proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]
            idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]
            min_size = min([len(idx_j) for idx_j in idx_batch])
    for j in range(num_users):
        dict_users[j] = idx_batch[j]

    train_data, test_data = [], []
    for i in range(num_users):
        indices = list(dict_users[i])
        if logger: logger.info(f'Client {i} labels: {np.unique(dataset_label[indices])}')
        X_train, X_test, y_train, y_test = train_test_split(dataset_image[indices], dataset_label[indices], train_size=0.8, shuffle=True)
        train_data.append([(torch.tensor(x), torch.tensor(y)) for x, y in zip(X_train, y_train)])
        test_data.append([(torch.tensor(x), torch.tensor(y)) for x, y in zip(X_test, y_test)])
    return train_data, test_data

def noniid_type10(datasetname, dataset, num_users, num_types, ratio, num_classes=10, logger=None):
    """
    [ ğŸ“ åšå£«è¯¾å ‚: é«˜ä¿çœŸé‡æ„ç‰ˆ ]
    é‡‡ç”¨çµæ´»çš„åˆ†é…ç­–ç•¥ï¼Œå…è®¸ num_types > num_usersï¼Œ
    ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½è¢«ä½¿ç”¨ï¼Œä¸”ä¸¥æ ¼éµå®ˆ num_types çš„åˆ‡åˆ†å®šä¹‰ã€‚
    """
    # 1. å¿«é€ŸåŠ è½½æ•°æ®
    trainloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=2)
    for _, train_data in enumerate(trainloader, 0):
        dataset_image, dataset_label = train_data

    # 2. å‡†å¤‡ "Dominant" (ä¸»è¦) å’Œ "Small" (å‰©ä½™éšæœº) æ•°æ®
    order = torch.randperm(dataset_image.shape[0])
    image_random = dataset_image[order]
    label_random = dataset_label[order]
    
    offset = int(dataset_image.shape[0] * ratio)
    image_class = image_random[:offset] # ä¸»è¦éƒ¨åˆ† (80%)
    label_class = label_random[:offset]
    image_s = image_random[offset:]     # å‰©ä½™éƒ¨åˆ† (20%)
    label_s = label_random[offset:]

    # 3. å¯¹ä¸»è¦æ•°æ®æŒ‰ç±»åˆ«æ’åºå¹¶åˆ‡åˆ†ä¸º num_types ä»½
    order = torch.argsort(label_class)
    x_sorted = image_class[order]
    y_sorted = label_class[order]
    
    # å¼ºåˆ¶åˆ‡åˆ†ä¸ºç”¨æˆ·è¦æ±‚çš„ num_types ä»½ (æ¯”å¦‚ 4 ä»½)
    x_shards = torch.tensor_split(x_sorted, num_types)
    y_shards = torch.tensor_split(y_sorted, num_types)

    # 4. åˆå§‹åŒ–æ¯ä¸ªç”¨æˆ·çš„å®¹å™¨
    # ä½¿ç”¨åˆ—è¡¨è€Œä¸æ˜¯ç›´æ¥ tensor è¿æ¥ï¼Œé¿å…é¢‘ç¹å†…å­˜æ‹·è´
    x_client_buckets = [[] for _ in range(num_users)]
    y_client_buckets = [[] for _ in range(num_users)]

    # 5. [æ ¸å¿ƒä¼˜åŒ–] æ™ºèƒ½åˆ†é…é€»è¾‘
    # æ— è®º shard å¤šè¿˜æ˜¯äººå¤šï¼Œéƒ½å…¬å¹³åˆ†é…
    for i in range(num_types):
        # å–å‡ºç¬¬ i ä¸ª shard (ä»£è¡¨æŸç§æ•°æ®åˆ†å¸ƒ)
        shard_x = x_shards[i]
        shard_y = y_shards[i]
        
        # ç­–ç•¥ï¼šå¦‚æœäººæ¯” shard å¤šï¼Œè¿™ä¸ª shard è¦æ‹†ç»™å¤šä¸ªäºº
        # å¦‚æœ shard æ¯”äººå¤šï¼Œä¸€ä¸ªäººè¦æ‹¿å¤šä¸ª shard (è½®è¯¢)
        
        if num_users > num_types:
            # è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ª shard è¦åˆ†ç»™ (num_users / num_types) ä¸ªäºº
            # è¿™é‡Œçš„è®¡ç®—æ¯”è¾ƒå¤æ‚ï¼Œä¸ºäº†ä¿æŒä»£ç æå…¶ç®€æ´ä¸”ç¨³å¥ï¼Œ
            # æˆ‘ä»¬ç›´æ¥ä½¿ç”¨æœ€é€šç”¨çš„ "å‘ç‰Œ" æ¨¡å¼ï¼š
            # å°† shard å†ç»†åˆ†ï¼Œå¡«è¡¥ç©ºç¼ºçš„ç”¨æˆ·
            
            # è®¡ç®—å½“å‰ shard åº”è¯¥è¦†ç›–å“ªäº›ç”¨æˆ·ç´¢å¼•
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ˜ å°„ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰ç”¨æˆ·
            sub_chunks = int(np.ceil(num_users / num_types))
            shard_x_parts = torch.tensor_split(shard_x, sub_chunks)
            shard_y_parts = torch.tensor_split(shard_y, sub_chunks)
            
            for j in range(len(shard_x_parts)):
                target_user = (i * sub_chunks + j) % num_users
                x_client_buckets[target_user].append(shard_x_parts[j])
                y_client_buckets[target_user].append(shard_y_parts[j])
        else:
            # [ç”¨æˆ·é‡åˆ°çš„æƒ…å†µ: N=3, T=4]
            # ç›´æ¥è½®è¯¢åˆ†é…ï¼šShard 0->U0, Shard 1->U1, Shard 2->U2, Shard 3->U0
            target_user = i % num_users
            x_client_buckets[target_user].append(shard_x)
            y_client_buckets[target_user].append(shard_y)

    # 6. åˆ†é…å‰©ä½™çš„ 20% éšæœºæ•°æ® (å‡åŒ€åˆ†é…)
    x_split_all = torch.tensor_split(image_s, num_users)
    y_split_all = torch.tensor_split(label_s, num_users)

    # 7. åˆå¹¶æœ€ç»ˆæ•°æ®
    data = []
    for i in range(num_users):
        # åˆå¹¶ Dominant éƒ¨åˆ†
        if len(x_client_buckets[i]) > 0:
            x_dom = torch.cat(x_client_buckets[i])
            y_dom = torch.cat(y_client_buckets[i])
        else:
            x_dom = torch.tensor([])
            y_dom = torch.tensor([])
            
        # åˆå¹¶ Random éƒ¨åˆ†
        X = torch.cat((x_dom, x_split_all[i]))
        y = torch.cat((y_dom, y_split_all[i]))
        
        if logger: 
            logger.info(f'Client {i} label types: {torch.unique(y)}')

        data.append([(x, y) for x, y in zip(X, y)])

    return data

def dirichlet_noniid(dataset, num_users=10, dirichlet_alpha=100, sample_matrix_test=None, test=False):
    train_labels = get_targets_safe(dataset)
    class_num = train_labels.max() + 1
    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}
    idxs = np.arange(len(train_labels))
    idxs_labels = np.vstack((idxs, train_labels))
    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]
    class_lableidx = [idxs_labels[:, idxs_labels[1, :] == i][0, :] for i in range(class_num)]

    if test is True and sample_matrix_test is not None:
        sample_matrix = sample_matrix_test
    else:
        sample_matrix = np.random.dirichlet([dirichlet_alpha for _ in range(num_users)], class_num).T
    class_sampe_start = [0 for i in range(class_num)]
    for i in range(num_users):
        rand_set, class_sampe_start = sample_rand(sample_matrix[i], class_lableidx, class_sampe_start)
        dict_users[i] = rand_set
    return dict_users, sample_matrix

def sample_rand(rand, class_lableidx, class_sampe_start):
    class_sampe_end = [start + int(len(class_lableidx[sidx]) * rand[sidx]) for sidx, start in enumerate(class_sampe_start)]
    rand_set = np.array([], dtype=np.int32)
    for eidx, rand_end in enumerate(class_sampe_end):
        rand_start = class_sampe_start[eidx]
        if rand_end <= len(class_lableidx[eidx]):
            rand_set = np.concatenate([rand_set, class_lableidx[eidx][rand_start:rand_end]], axis=0)
        else:
            if rand_start < len(class_lableidx[eidx]):
                rand_set = np.concatenate([rand_set, class_lableidx[eidx][rand_start:]], axis=0)
            else:
                if len(class_lableidx[eidx]) > 0:
                     rand_set = np.concatenate([rand_set, random.sample(list(class_lableidx[eidx]), min(len(class_lableidx[eidx]), rand_end - rand_start + 1))], axis=0)
    if rand_set.shape[0] == 0:
        rand_set = np.concatenate([rand_set, class_lableidx[0][0:1]], axis=0)
    return rand_set, class_sampe_end

def data_loader(datasetname, trainset, testset):
    trainloader = DataLoader(trainset, batch_size=len(trainset), shuffle=False, num_workers=2)
    testloader = DataLoader(testset, batch_size=len(testset), shuffle=False, num_workers=2)
    dataset_image, dataset_label = [], []
    for _, (data, target) in enumerate(trainloader):
        dataset_image.extend(np.array(data))
        dataset_label.extend(np.array(target))
    for _, (data, target) in enumerate(testloader):
        dataset_image.extend(np.array(data))
        dataset_label.extend(np.array(target))
    return np.array(dataset_image), np.array(dataset_label)